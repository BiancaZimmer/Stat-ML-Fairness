{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RepresentativenessFairness.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgm-ocn_6U4R"
      },
      "source": [
        "# How to detect and mitigate Bias in Machine Learning\n",
        "For the Seminar \"Statistical Machine Learning\" at the Otto-Friedrich University of Bamberg Louisa Heidrich and Bianca Zimmer chose to investigate the topic Representativeness vs. Fairness. For the theoretical background please see our presentention.   \n",
        "<br />\n",
        "To demonstrate some of the fairness methods listed in our presentation we decided to use an open-source data set about American homicide data (https://www.kaggle.com/murderaccountability/homicide-reports). The data set contains information about location, time and victim of the crime. If the crime was solved we also have information about the perpetrator and their relationship to the victim."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HrUWmg2PQpB"
      },
      "source": [
        "### Table of Contents\n",
        "\n",
        "* [0. Load packages and global variables](#chapter0)\n",
        "* [1. Introduction](#chapter1)\n",
        "* [2. AIF360 Tool](#chapter2)\n",
        "* [3. Bias Detection](#chapter3)\n",
        "* [4. Bias Mitigation](#chapter4)\n",
        "* [5. Results of applying the AIF360 Tool](#chapter5)\n",
        "* [6. Conclusion](#chapter6)\n",
        "* [7. References](#chapter7)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjOl6977TdrR"
      },
      "source": [
        "# 0. Load packages and global variables <a class=\"anchor\" id=\"chapter0\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_hwWcrmw32Y"
      },
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "plt.style.use('seaborn-white')\n",
        "import seaborn as sns\n",
        "import plotly.offline as py\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "from IPython.display import Markdown, display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "Na3O-0KYXD_I",
        "outputId": "a65419d8-a35d-42dd-f449-7564a230dab5"
      },
      "source": [
        "import sys\n",
        "sys.version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3.7.11 (default, Jul  3 2021, 18:01:19) \\n[GCC 7.5.0]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZxu_gHA6U4U"
      },
      "source": [
        "# 1. Introduction <a class=\"anchor\" id=\"chapter1\"></a>\n",
        "\n",
        "2016 Microsoft started a chatbot on Twitter named Tay. Tay was a machine learning experiment. She should learn how to communicate naturally with people through social media. However nobody was ready for what happened next: Tay turned into a racist in less than 24 hours.   \n",
        "This incident started a debate about what machine learning algorithms have to do with fairness and who is responsible for the (un-)fairness of models. Bias per se has been in the discussion for a long time when speaking about models in general but fairness takes this whole concept a step into the ethical direction. A quote which describes this situation perfectly by Barocas et al. 2017 reads as follows:   \n",
        "\n",
        "```\n",
        "Our historical examples of the relevant outcomes will almost always reflect historical prejudices\n",
        "against certain social groups, prevailing cultural stereotypes, and existing demographic inequalities.\n",
        "And finding patterns in these data will often mean replicating these very same dynamics. (Barocas et al. 2017)\n",
        "```\n",
        "The first problem we come across is the question \"How do we measure fairness?\". This a very difficult and nearly philosophical question which is reflected by the fact that there are over 20 measurements for fairness.   \n",
        "The next question is: \"How can we mitigate that bias in fairness while still producing a valid model?\". There have been approaches to mitigate bias mathematically in different stages of the model finding process all with a different focus.  \n",
        "<br /> \n",
        "In the course of this project we will try to answer some of these questions by\n",
        "* Introducing some fairness metrics\n",
        "* Applying methods to mitigate bias before, during and after finding a model\n",
        "* Testing the AIF360 toolkit by IBM on a real life dataset (Homicide Data)\n",
        "\n",
        "<br /> \n",
        "It is important to notice that the aim of this project is not to find the best prediction model. However we will apply some baseline data quality review and clean up in order to get a fairly useful model. We decided on predicting whether a crime was solved or not depending on the remaining available variables in the homicide dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI1WZ91q6U4U"
      },
      "source": [
        "### 1.1 Load and review dataset \"U.S. Homicide Reports\" (1980-2014)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kAhdA1FEqnd"
      },
      "source": [
        "First of all, let's have a look at the dataset. It consists of 638,454 observations on homicides in the U.S. between 1980 and 2014. For this purpose 24 attributes were collected and are listed below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "9pN1-CUY6U4V",
        "outputId": "db627f83-5c55-46dc-dd62-a9d87f3724b2"
      },
      "source": [
        "#load data set from github repository\n",
        "url = 'https://github.com/BiancaZimmer/Stat-ML-Fairness/raw/2d723889786d615e656c87c479959ee143176c65/data.zip' #change blob to raw in perma github link to make it work\n",
        "data = pd.read_csv(url, compression = 'zip',na_values = ['Unknown',' '])\n",
        "display(Markdown(\"#### Shape of Data\"))\n",
        "print(data.shape)\n",
        "display(Markdown(\"#### Column names\"))\n",
        "print(data.columns)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Shape of Data",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "(638454, 24)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Column names",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Index(['Record ID', 'Agency Code', 'Agency Name', 'Agency Type', 'City',\n",
            "       'State', 'Year', 'Month', 'Incident', 'Crime Type', 'Crime Solved',\n",
            "       'Victim Sex', 'Victim Age', 'Victim Race', 'Victim Ethnicity',\n",
            "       'Perpetrator Sex', 'Perpetrator Age', 'Perpetrator Race',\n",
            "       'Perpetrator Ethnicity', 'Relationship', 'Weapon', 'Victim Count',\n",
            "       'Perpetrator Count', 'Record Source'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Record ID</th>\n",
              "      <th>Agency Code</th>\n",
              "      <th>Agency Name</th>\n",
              "      <th>Agency Type</th>\n",
              "      <th>City</th>\n",
              "      <th>State</th>\n",
              "      <th>Year</th>\n",
              "      <th>Month</th>\n",
              "      <th>Incident</th>\n",
              "      <th>Crime Type</th>\n",
              "      <th>Crime Solved</th>\n",
              "      <th>Victim Sex</th>\n",
              "      <th>Victim Age</th>\n",
              "      <th>Victim Race</th>\n",
              "      <th>Victim Ethnicity</th>\n",
              "      <th>Perpetrator Sex</th>\n",
              "      <th>Perpetrator Age</th>\n",
              "      <th>Perpetrator Race</th>\n",
              "      <th>Perpetrator Ethnicity</th>\n",
              "      <th>Relationship</th>\n",
              "      <th>Weapon</th>\n",
              "      <th>Victim Count</th>\n",
              "      <th>Perpetrator Count</th>\n",
              "      <th>Record Source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>AK00101</td>\n",
              "      <td>Anchorage</td>\n",
              "      <td>Municipal Police</td>\n",
              "      <td>Anchorage</td>\n",
              "      <td>Alaska</td>\n",
              "      <td>1980</td>\n",
              "      <td>January</td>\n",
              "      <td>1</td>\n",
              "      <td>Murder or Manslaughter</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Male</td>\n",
              "      <td>14</td>\n",
              "      <td>Native American/Alaska Native</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Male</td>\n",
              "      <td>15.0</td>\n",
              "      <td>Native American/Alaska Native</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Acquaintance</td>\n",
              "      <td>Blunt Object</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>FBI</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>AK00101</td>\n",
              "      <td>Anchorage</td>\n",
              "      <td>Municipal Police</td>\n",
              "      <td>Anchorage</td>\n",
              "      <td>Alaska</td>\n",
              "      <td>1980</td>\n",
              "      <td>March</td>\n",
              "      <td>1</td>\n",
              "      <td>Murder or Manslaughter</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Male</td>\n",
              "      <td>43</td>\n",
              "      <td>White</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Male</td>\n",
              "      <td>42.0</td>\n",
              "      <td>White</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Acquaintance</td>\n",
              "      <td>Strangulation</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>FBI</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>AK00101</td>\n",
              "      <td>Anchorage</td>\n",
              "      <td>Municipal Police</td>\n",
              "      <td>Anchorage</td>\n",
              "      <td>Alaska</td>\n",
              "      <td>1980</td>\n",
              "      <td>March</td>\n",
              "      <td>2</td>\n",
              "      <td>Murder or Manslaughter</td>\n",
              "      <td>No</td>\n",
              "      <td>Female</td>\n",
              "      <td>30</td>\n",
              "      <td>Native American/Alaska Native</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>FBI</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>AK00101</td>\n",
              "      <td>Anchorage</td>\n",
              "      <td>Municipal Police</td>\n",
              "      <td>Anchorage</td>\n",
              "      <td>Alaska</td>\n",
              "      <td>1980</td>\n",
              "      <td>April</td>\n",
              "      <td>1</td>\n",
              "      <td>Murder or Manslaughter</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Male</td>\n",
              "      <td>43</td>\n",
              "      <td>White</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Male</td>\n",
              "      <td>42.0</td>\n",
              "      <td>White</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Acquaintance</td>\n",
              "      <td>Strangulation</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>FBI</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>AK00101</td>\n",
              "      <td>Anchorage</td>\n",
              "      <td>Municipal Police</td>\n",
              "      <td>Anchorage</td>\n",
              "      <td>Alaska</td>\n",
              "      <td>1980</td>\n",
              "      <td>April</td>\n",
              "      <td>2</td>\n",
              "      <td>Murder or Manslaughter</td>\n",
              "      <td>No</td>\n",
              "      <td>Female</td>\n",
              "      <td>30</td>\n",
              "      <td>Native American/Alaska Native</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>FBI</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Record ID Agency Code  ... Perpetrator Count Record Source\n",
              "0          1     AK00101  ...                 0           FBI\n",
              "1          2     AK00101  ...                 0           FBI\n",
              "2          3     AK00101  ...                 0           FBI\n",
              "3          4     AK00101  ...                 0           FBI\n",
              "4          5     AK00101  ...                 1           FBI\n",
              "\n",
              "[5 rows x 24 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avKWI6tCzAVS"
      },
      "source": [
        "## 1.2 Clean up\n",
        "\n",
        "The attribute `Agency Code` can be dropped, because it only represents the `Agency Name` as a code and is thus duplicate information.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZy8iWn1zFyw"
      },
      "source": [
        "data_orig = data.copy() #saving a copy of the original data\n",
        "cols_to_drop = ['Agency Code']\n",
        "data.drop(columns=cols_to_drop, inplace=True)\n",
        "cols_to_drop = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sZvWizJ-rch"
      },
      "source": [
        "**Next we will have a look at all missing values.**\n",
        "* The graphics below show that especially the attribute `Ethnicity` has the most missing values for both perpetrator and victim. This problem can be eliminated by dropping the attribute ethnicity. We can easily do that because it refers to people being hispanic or not which is not relevant for our analyses. In the following **we rather focus on the protected attribute race** differentiating between privileged white people and unprivileged People of Colour.    \n",
        "* The attribute `Relationship` also shows high missing values. It refers to the relationship between the perpetrator and victim. Looking at the possible outcomes one can see that **relationship seems to be a not well defined attribute** since some of the categories are overlapping (e.g. Boyfriend/Girlfriend and Boyfriend and Girlfriend). For a first analysis this attribute will also be excluded due to poor data quality.    \n",
        "* Unfortunately, the missing values for the attributes `Perpetrator Race` and `Perpetrator Sex` are also comparatively high. We notice that **in the case of an unsolved crime we never have any information about the perpetrator's sex or race**. When we want to predict whether a crime was solved or not we will have to exclude these two attributes in order to get a valid model.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d_5-1_YX0QD2",
        "outputId": "7a75f8fc-1f1e-424c-fdbf-72c851af27fc"
      },
      "source": [
        "def print_missing_values(data):\n",
        "    data_null = pd.DataFrame(len(data) - data.notnull().sum(), columns = ['Count'])\n",
        "    data_null = data_null[data_null['Count'] > 0].sort_values(by='Count', ascending=False)\n",
        "    data_null = data_null/len(data)*100\n",
        "    #return(data_null)\n",
        "    trace = go.Bar(x=data_null.index, y=data_null['Count'])\n",
        "    layout = go.Layout(title='Column with at least one missing value', yaxis=dict(title='Percentage of missing values'))\n",
        "    fig = go.Figure([trace], layout=layout)\n",
        "    py.iplot(fig)\n",
        "\n",
        "display(Markdown(\"#### All data\"))\n",
        "print_missing_values(data)\n",
        "display(Markdown(\"#### Where crime was solved\"))\n",
        "print_missing_values(data[data['Crime Solved']=='Yes'])\n",
        "display(Markdown(\"#### Where crime was not solved\"))\n",
        "print_missing_values(data[data['Crime Solved']=='No'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### All data",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"25d2efd1-b0c9-48ce-a0b9-789d91006de2\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"25d2efd1-b0c9-48ce-a0b9-789d91006de2\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '25d2efd1-b0c9-48ce-a0b9-789d91006de2',\n",
              "                        [{\"type\": \"bar\", \"x\": [\"Perpetrator Ethnicity\", \"Victim Ethnicity\", \"Relationship\", \"Perpetrator Race\", \"Perpetrator Sex\", \"Weapon\", \"Victim Race\", \"Victim Sex\", \"Agency Name\", \"Perpetrator Age\"], \"y\": [69.92046412114263, 57.6866931681844, 42.761577184887244, 30.70651918540725, 29.816556870189554, 5.198808371472338, 1.045650900456415, 0.15412230168500785, 0.007361532702434319, 0.00015662835537094292]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Column with at least one missing value\"}, \"yaxis\": {\"title\": {\"text\": \"Percentage of missing values\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('25d2efd1-b0c9-48ce-a0b9-789d91006de2');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Where crime was solved",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"014baff3-b116-42f9-8d95-af509b2ee27b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"014baff3-b116-42f9-8d95-af509b2ee27b\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '014baff3-b116-42f9-8d95-af509b2ee27b',\n",
              "                        [{\"type\": \"bar\", \"x\": [\"Perpetrator Ethnicity\", \"Victim Ethnicity\", \"Relationship\", \"Weapon\", \"Perpetrator Race\", \"Victim Race\", \"Victim Sex\", \"Perpetrator Sex\", \"Agency Name\"], \"y\": [57.20437689101505, 56.90493828262364, 21.173121033888776, 4.1075747704006496, 1.3497050239640138, 0.8014780039806145, 0.0738555733066769, 0.032799907178493966, 0.007363244468641504]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Column with at least one missing value\"}, \"yaxis\": {\"title\": {\"text\": \"Percentage of missing values\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('014baff3-b116-42f9-8d95-af509b2ee27b');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Where crime was not solved",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"6cbc4698-5aa5-4c61-823b-3d384d02bcf9\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"6cbc4698-5aa5-4c61-823b-3d384d02bcf9\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '6cbc4698-5aa5-4c61-823b-3d384d02bcf9',\n",
              "                        [{\"type\": \"bar\", \"x\": [\"Perpetrator Sex\", \"Perpetrator Ethnicity\", \"Perpetrator Race\", \"Relationship\", \"Victim Ethnicity\", \"Weapon\", \"Victim Race\", \"Victim Sex\", \"Agency Name\", \"Perpetrator Age\"], \"y\": [99.96636570984118, 99.87071819720205, 99.85074783742024, 93.60895933404106, 59.52796375905235, 7.768995490902976, 1.62075235702799, 0.3431748667766788, 0.0073575009722412, 0.0005255357837315143]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Column with at least one missing value\"}, \"yaxis\": {\"title\": {\"text\": \"Percentage of missing values\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('6cbc4698-5aa5-4c61-823b-3d384d02bcf9');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfzYCQ-zrIBn"
      },
      "source": [
        "cols_to_drop = ['Victim Ethnicity',  'Perpetrator Ethnicity', 'Perpetrator Sex','Perpetrator Race', 'Relationship']\n",
        "data.drop(columns=cols_to_drop, inplace=True)\n",
        "cols_to_drop = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azp3BngbCoud"
      },
      "source": [
        "We save an array for the numerical and for the categorical columns. This will come in handy later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "QP6ClsSXrNu6",
        "outputId": "215ab673-20e2-464b-ed58-04d5f267544e"
      },
      "source": [
        "columns_categorical = ['Agency Name',\n",
        " 'Agency Type',\n",
        " 'City',\n",
        " 'State',\n",
        " 'Month',\n",
        " 'Crime Type',\n",
        " 'Victim Sex',\n",
        " 'Victim Race',\n",
        " 'Weapon',\n",
        " 'Record Source',\n",
        " 'Crime Solved',\n",
        " 'Victim Count',\n",
        " 'Perpetrator Count']\n",
        "\n",
        "columns_numerical = [c for c in data.columns.values if c not in columns_categorical][1:] #'Perpetrator Age' is quasi-numerical can be seen as categorical or not ...\n",
        "\n",
        "display(Markdown(\"#### Categorical columns\"))\n",
        "print(columns_categorical)\n",
        "display(Markdown(\"#### Numerical columns\"))\n",
        "columns_numerical"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Categorical columns",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "['Agency Name', 'Agency Type', 'City', 'State', 'Month', 'Crime Type', 'Victim Sex', 'Victim Race', 'Weapon', 'Record Source', 'Crime Solved', 'Victim Count', 'Perpetrator Count']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Numerical columns",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Year', 'Incident', 'Victim Age', 'Perpetrator Age']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_NGROAkCyNz"
      },
      "source": [
        "To get an overview of the values of the categorical columns we print the respective value counts for each column. In the notebook we only print the TOP 10 to limit the printout. However it's possible to change this in line 3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vXLAzZGAp8tF",
        "outputId": "a415b730-08fa-47d3-f86f-46edf380ce83"
      },
      "source": [
        "for column in columns_categorical:\n",
        "  print(column)\n",
        "  print(data[column].value_counts()[:11])\n",
        "  display(Markdown(\"#### -----\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Agency Name\n",
            "New York        38416\n",
            "Los Angeles     29007\n",
            "Chicago         21331\n",
            "Detroit         17206\n",
            "Houston         13046\n",
            "Philadelphia    12861\n",
            "Dallas           9611\n",
            "Baltimore        9331\n",
            "New Orleans      7846\n",
            "Washington       7828\n",
            "St. Louis        5981\n",
            "Name: Agency Name, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### -----",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Agency Type\n",
            "Municipal Police    493026\n",
            "Sheriff             105322\n",
            "County Police        22693\n",
            "State Police         14235\n",
            "Special Police        2889\n",
            "Regional Police        235\n",
            "Tribal Police           54\n",
            "Name: Agency Type, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### -----",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "City\n",
            "Los Angeles       44511\n",
            "New York          38431\n",
            "Cook              22383\n",
            "Wayne             19904\n",
            "Harris            16331\n",
            "Philadelphia      12851\n",
            "Dallas            11377\n",
            "Jefferson          9573\n",
            "Baltimore city     9336\n",
            "Dade               9077\n",
            "Maricopa           8582\n",
            "Name: City, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### -----",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "State\n",
            "California        99783\n",
            "Texas             62095\n",
            "New York          49268\n",
            "Florida           37164\n",
            "Michigan          28448\n",
            "Illinois          25871\n",
            "Pennsylvania      24236\n",
            "Georgia           21088\n",
            "North Carolina    20390\n",
            "Louisiana         19629\n",
            "Ohio              19158\n",
            "Name: State, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### -----",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Month\n",
            "July         58696\n",
            "August       58072\n",
            "December     55187\n",
            "September    54117\n",
            "June         53662\n",
            "October      53650\n",
            "May          53394\n",
            "January      52928\n",
            "March        51444\n",
            "April        51209\n",
            "November     50016\n",
            "Name: Month, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### -----",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Crime Type\n",
            "Murder or Manslaughter        629338\n",
            "Manslaughter by Negligence      9116\n",
            "Name: Crime Type, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### -----",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Victim Sex\n",
            "Male      494125\n",
            "Female    143345\n",
            "Name: Victim Sex, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### -----",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Victim Race\n",
            "White                            317422\n",
            "Black                            299899\n",
            "Asian/Pacific Islander             9890\n",
            "Native American/Alaska Native      4567\n",
            "Name: Victim Race, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### -----",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Weapon\n",
            "Handgun          317484\n",
            "Knife             94962\n",
            "Blunt Object      67337\n",
            "Firearm           46980\n",
            "Shotgun           30722\n",
            "Rifle             23347\n",
            "Strangulation      8110\n",
            "Fire               6173\n",
            "Suffocation        3968\n",
            "Gun                2206\n",
            "Drugs              1588\n",
            "Name: Weapon, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### -----",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Record Source\n",
            "FBI     616647\n",
            "FOIA     21807\n",
            "Name: Record Source, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### -----",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Crime Solved\n",
            "Yes    448172\n",
            "No     190282\n",
            "Name: Crime Solved, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### -----",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Victim Count\n",
            "0     586059\n",
            "1      38750\n",
            "2       8156\n",
            "3       2847\n",
            "4       1084\n",
            "5        510\n",
            "9        290\n",
            "6        286\n",
            "7        168\n",
            "10       160\n",
            "8        144\n",
            "Name: Victim Count, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### -----",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Perpetrator Count\n",
            "0     558838\n",
            "1      54745\n",
            "2      15777\n",
            "3       6531\n",
            "4       1489\n",
            "5        592\n",
            "6        207\n",
            "7        129\n",
            "9         52\n",
            "8         52\n",
            "10        42\n",
            "Name: Perpetrator Count, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### -----",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0dOx-2eDHBW"
      },
      "source": [
        "To get an overview of the numerical columns we print the discription for each column and a histogram to have a closer look at the column `Incident`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 913
        },
        "id": "i1dvsRmR5cIE",
        "outputId": "d5c028c0-c545-4d09-dfe9-d24e3f12fbec"
      },
      "source": [
        "print(data[columns_numerical].describe())\n",
        "\n",
        "display(Markdown(\"#### Number of Victim Age = 0 \"))\n",
        "print(data[data['Victim Age'] == 0].shape[0])\n",
        "print(pd.crosstab(data['Crime Solved'], data['Victim Age']==0, normalize='index'))\n",
        "\n",
        "display(Markdown(\"#### Number of Victim Age > 99 \"))\n",
        "print(data[data['Victim Age'] > 99].shape[0])\n",
        "print(data[data['Victim Age'] == 998].shape[0])\n",
        "print(pd.crosstab(data['Crime Solved'], data['Victim Age']>99, normalize='index'))\n",
        "\n",
        "display(Markdown(\"#### Number of Perpetrator Age = 0 \"))\n",
        "print(data[data['Perpetrator Age'] == 0].shape[0])\n",
        "print(pd.crosstab(data['Crime Solved'], data['Perpetrator Age']==0, normalize='index'))\n",
        "\n",
        "display(Markdown(\"#### Histogram for Incident > 199 to get a better overview of the column's structure \"))\n",
        "plt.hist(data[data['Incident'] > 199]['Incident'], bins= 8) # also see without restriction on incident and bins=10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                Year       Incident     Victim Age  Perpetrator Age\n",
            "count  638454.000000  638454.000000  638454.000000    638453.000000\n",
            "mean     1995.801102      22.967924      35.033512        20.322697\n",
            "std         9.927693      92.149821      41.628306        17.886842\n",
            "min      1980.000000       0.000000       0.000000         0.000000\n",
            "25%      1987.000000       1.000000      22.000000         0.000000\n",
            "50%      1995.000000       2.000000      30.000000        21.000000\n",
            "75%      2004.000000      10.000000      42.000000        31.000000\n",
            "max      2014.000000     999.000000     998.000000        99.000000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Number of Victim Age = 0 ",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "8444\n",
            "Victim Age       False     True \n",
            "Crime Solved                    \n",
            "No            0.994613  0.005387\n",
            "Yes           0.983446  0.016554\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Number of Victim Age > 99 ",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "974\n",
            "974\n",
            "Victim Age       False     True \n",
            "Crime Solved                    \n",
            "No            0.996821  0.003179\n",
            "Yes           0.999177  0.000823\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Number of Perpetrator Age = 0 ",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "216327\n",
            "Perpetrator Age     False     True \n",
            "Crime Solved                       \n",
            "No               0.003442  0.996558\n",
            "Yes              0.940425  0.059575\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Histogram for Incident > 199 to get a better overview of the column's structure ",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1992., 1712., 1707., 1640., 1605., 1601., 1603., 1604.]),\n",
              " array([200.   , 299.875, 399.75 , 499.625, 599.5  , 699.375, 799.25 ,\n",
              "        899.125, 999.   ]),\n",
              " <a list of 8 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhjQZkZcT6VW"
      },
      "source": [
        "* **`Incident` seems to have a rather curious data structure**: A lot of the distribution is around 0 to 100 with some between 100 and 200 and then we have a nearly uniform distribution between 200 and 999. Since we could not find any documentation on what the incident attribute stands for, we will not include it into our model to prevent any unexplainable bias.\n",
        "* There are **few victims with ages equal to 0**. We will not change this to NaN values since it is still possible that a baby was murdered. Furthermore the distribution of age = 0 between solved and unsolved crime seems to be logical with three times more solved crimes than unsolved crimes.\n",
        "* There are 974 observations where the **victim age is equal to 998**. This is four-times more likely when the crime was not solved. We assume that these are cases of wrongly attributed ages and will set those values to NaN\n",
        "* We notice that about a third of all the `Perpetrator Age` is set to 0. With 99.6% of unsolved crimes having a perpetrator's age of 0. It seems very unlikely that babies are responsible for so many murders. It's rather plausible that instead of coding `Perpetrator Age` with NaN it was coded with 0 in this data set. That is why **we will have to exclude `Perpetrator Age`** in our model for the solved crime as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVd62Ww88aqF",
        "outputId": "e9a7bd63-779b-4195-9c15-cd7912406544"
      },
      "source": [
        "cols_to_drop = ['Perpetrator Age','Incident']\n",
        "data.drop(columns=cols_to_drop, inplace=True)\n",
        "columns_numerical.remove('Perpetrator Age')\n",
        "columns_numerical.remove('Incident')\n",
        "cols_to_drop = []\n",
        "\n",
        "data[data['Victim Age'] > 99] = np.nan\n",
        "print(data[columns_numerical].describe())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "               Year     Victim Age\n",
            "count  637480.00000  637480.000000\n",
            "mean     1995.79507      33.562204\n",
            "std         9.92599      17.792594\n",
            "min      1980.00000       0.000000\n",
            "25%      1987.00000      22.000000\n",
            "50%      1995.00000      30.000000\n",
            "75%      2004.00000      42.000000\n",
            "max      2014.00000      99.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s4PPqfmrRlZ"
      },
      "source": [
        "To make our lives easier we will now add a column `Race` denoting People of Color. This attribute will be encoded with `White`, `PoC`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "QD8ervT7rRx9",
        "outputId": "54ee0f9d-ccae-4e92-9c69-b92cd6c24e7d"
      },
      "source": [
        "data['Race'] = np.where(data['Victim Race'] == 'White', 'White', 'PoC')\n",
        "columns_categorical = columns_categorical + ['Race']\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Record ID</th>\n",
              "      <th>Agency Name</th>\n",
              "      <th>Agency Type</th>\n",
              "      <th>City</th>\n",
              "      <th>State</th>\n",
              "      <th>Year</th>\n",
              "      <th>Month</th>\n",
              "      <th>Crime Type</th>\n",
              "      <th>Crime Solved</th>\n",
              "      <th>Victim Sex</th>\n",
              "      <th>Victim Age</th>\n",
              "      <th>Victim Race</th>\n",
              "      <th>Weapon</th>\n",
              "      <th>Victim Count</th>\n",
              "      <th>Perpetrator Count</th>\n",
              "      <th>Record Source</th>\n",
              "      <th>Race</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>Anchorage</td>\n",
              "      <td>Municipal Police</td>\n",
              "      <td>Anchorage</td>\n",
              "      <td>Alaska</td>\n",
              "      <td>1980.0</td>\n",
              "      <td>January</td>\n",
              "      <td>Murder or Manslaughter</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Male</td>\n",
              "      <td>14.0</td>\n",
              "      <td>Native American/Alaska Native</td>\n",
              "      <td>Blunt Object</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>FBI</td>\n",
              "      <td>PoC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>Anchorage</td>\n",
              "      <td>Municipal Police</td>\n",
              "      <td>Anchorage</td>\n",
              "      <td>Alaska</td>\n",
              "      <td>1980.0</td>\n",
              "      <td>March</td>\n",
              "      <td>Murder or Manslaughter</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Male</td>\n",
              "      <td>43.0</td>\n",
              "      <td>White</td>\n",
              "      <td>Strangulation</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>FBI</td>\n",
              "      <td>White</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.0</td>\n",
              "      <td>Anchorage</td>\n",
              "      <td>Municipal Police</td>\n",
              "      <td>Anchorage</td>\n",
              "      <td>Alaska</td>\n",
              "      <td>1980.0</td>\n",
              "      <td>March</td>\n",
              "      <td>Murder or Manslaughter</td>\n",
              "      <td>No</td>\n",
              "      <td>Female</td>\n",
              "      <td>30.0</td>\n",
              "      <td>Native American/Alaska Native</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>FBI</td>\n",
              "      <td>PoC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.0</td>\n",
              "      <td>Anchorage</td>\n",
              "      <td>Municipal Police</td>\n",
              "      <td>Anchorage</td>\n",
              "      <td>Alaska</td>\n",
              "      <td>1980.0</td>\n",
              "      <td>April</td>\n",
              "      <td>Murder or Manslaughter</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Male</td>\n",
              "      <td>43.0</td>\n",
              "      <td>White</td>\n",
              "      <td>Strangulation</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>FBI</td>\n",
              "      <td>White</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>Anchorage</td>\n",
              "      <td>Municipal Police</td>\n",
              "      <td>Anchorage</td>\n",
              "      <td>Alaska</td>\n",
              "      <td>1980.0</td>\n",
              "      <td>April</td>\n",
              "      <td>Murder or Manslaughter</td>\n",
              "      <td>No</td>\n",
              "      <td>Female</td>\n",
              "      <td>30.0</td>\n",
              "      <td>Native American/Alaska Native</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>FBI</td>\n",
              "      <td>PoC</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Record ID Agency Name  ... Record Source   Race\n",
              "0        1.0   Anchorage  ...           FBI    PoC\n",
              "1        2.0   Anchorage  ...           FBI  White\n",
              "2        3.0   Anchorage  ...           FBI    PoC\n",
              "3        4.0   Anchorage  ...           FBI  White\n",
              "4        5.0   Anchorage  ...           FBI    PoC\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hETHrfozrMcI"
      },
      "source": [
        "# 2. AIF360 Tool <a class=\"anchor\" id=\"chapter2\"></a>\n",
        "\n",
        "In our investigation we will apply the metrics and bias mitigation algorithms of the AIF360 toolkit by Bellamy et al.: \n",
        "\"This extensible open source tool kit can help you examine, report, and mitigate discrimination and bias in machine learning models throughout the AI application lifecycle\" (Bellamy et al. 2018). \\\\\n",
        "The code is available in Python and R. If you want to know more about how to use the toolkit please have a look at IBM's official website listed below.   \n",
        "In this project we rely on the documentation and demonstration code given in the GitHub repository.   \n",
        "   \n",
        "Official website: https://aif360.mybluemix.net.  \n",
        "Documentation can be found here: https://aif360.readthedocs.io/en/latest/index.html   \n",
        "Official GitHub repository can be found here: https://github.com/Trusted-AI/AIF360  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WDSJGoauCKS"
      },
      "source": [
        "## 2.0 Installing the AIF360 Tool & Loading respective packages\n",
        "Since it is not a standard in the google colab environment, we need to install AIF360 first. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2l7hGkwOstpN",
        "outputId": "f53597ba-4f5a-49a5-f86f-720d5b0ef45b"
      },
      "source": [
        "!pip install 'aif360[all]'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting aif360[all]\n",
            "  Downloading aif360-0.4.0-py3-none-any.whl (175 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 35.6 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 27.9 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 40 kB 22.9 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 51 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 61 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 81 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 92 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 102 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 112 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 122 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 133 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 143 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 153 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 163 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 174 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 175 kB 12.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from aif360[all]) (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from aif360[all]) (3.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from aif360[all]) (1.1.5)\n",
            "Collecting tempeh\n",
            "  Downloading tempeh-0.1.12-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: scipy<1.6.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from aif360[all]) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from aif360[all]) (0.22.2.post1)\n",
            "Collecting fairlearn==0.4.6\n",
            "  Downloading fairlearn-0.4.6-py3-none-any.whl (21.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.2 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from aif360[all]) (1.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from aif360[all]) (4.41.1)\n",
            "Collecting sphinx-rtd-theme\n",
            "  Downloading sphinx_rtd_theme-0.5.2-py2.py3-none-any.whl (9.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1 MB 80.2 MB/s \n",
            "\u001b[?25hCollecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[K     |████████████████████████████████| 275 kB 73.9 MB/s \n",
            "\u001b[?25hCollecting adversarial-robustness-toolbox>=1.0.0\n",
            "  Downloading adversarial_robustness_toolbox-1.7.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 52.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sphinx in /usr/local/lib/python3.7/dist-packages (from aif360[all]) (1.8.5)\n",
            "Requirement already satisfied: pytest>=3.5 in /usr/local/lib/python3.7/dist-packages (from aif360[all]) (3.6.4)\n",
            "Collecting BlackBoxAuditing\n",
            "  Downloading BlackBoxAuditing-0.1.54.tar.gz (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 58.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cvxpy>=1.0 in /usr/local/lib/python3.7/dist-packages (from aif360[all]) (1.0.31)\n",
            "Requirement already satisfied: tensorflow>=1.13.1 in /usr/local/lib/python3.7/dist-packages (from aif360[all]) (2.5.0)\n",
            "Requirement already satisfied: ipywidgets>=7.5.0 in /usr/local/lib/python3.7/dist-packages (from fairlearn==0.4.6->aif360[all]) (7.6.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox>=1.0.0->aif360[all]) (57.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from adversarial-robustness-toolbox>=1.0.0->aif360[all]) (1.15.0)\n",
            "Collecting numba~=0.53.1\n",
            "  Downloading numba-0.53.1-cp37-cp37m-manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 51.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: osqp>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from cvxpy>=1.0->aif360[all]) (0.6.2.post0)\n",
            "Requirement already satisfied: scs>=1.1.3 in /usr/local/lib/python3.7/dist-packages (from cvxpy>=1.0->aif360[all]) (2.1.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from cvxpy>=1.0->aif360[all]) (0.70.12.2)\n",
            "Requirement already satisfied: ecos>=2 in /usr/local/lib/python3.7/dist-packages (from cvxpy>=1.0->aif360[all]) (2.0.7.post1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (1.0.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (3.5.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (5.1.3)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (4.10.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (5.5.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (5.0.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (5.3.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (0.8.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (4.8.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (2.6.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (4.7.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (0.2.0)\n",
            "Collecting llvmlite<0.37,>=0.36.0rc1\n",
            "  Downloading llvmlite-0.36.0-cp37-cp37m-manylinux2010_x86_64.whl (25.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 60 kB/s \n",
            "\u001b[?25hRequirement already satisfied: qdldl in /usr/local/lib/python3.7/dist-packages (from osqp>=0.4.1->cvxpy>=1.0->aif360[all]) (0.1.5.post0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->aif360[all]) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->aif360[all]) (2018.9)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (0.2.5)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=3.5->aif360[all]) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=3.5->aif360[all]) (21.2.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest>=3.5->aif360[all]) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=3.5->aif360[all]) (8.8.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=3.5->aif360[all]) (1.10.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->aif360[all]) (1.0.1)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.13.1->aif360[all]) (1.12)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.13.1->aif360[all]) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.13.1->aif360[all]) (3.7.4.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.13.1->aif360[all]) (1.1.2)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.13.1->aif360[all]) (1.6.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.13.1->aif360[all]) (3.17.3)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.13.1->aif360[all]) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.13.1->aif360[all]) (3.1.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.13.1->aif360[all]) (1.34.1)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.13.1->aif360[all]) (0.36.2)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.13.1->aif360[all]) (2.5.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.13.1->aif360[all]) (3.3.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.13.1->aif360[all]) (0.12.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.13.1->aif360[all]) (1.1.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.13.1->aif360[all]) (0.2.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.13.1->aif360[all]) (0.4.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.13.1->aif360[all]) (1.12.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow>=1.13.1->aif360[all]) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=1.13.1->aif360[all]) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=1.13.1->aif360[all]) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=1.13.1->aif360[all]) (1.32.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=1.13.1->aif360[all]) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=1.13.1->aif360[all]) (0.4.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=1.13.1->aif360[all]) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=1.13.1->aif360[all]) (2.23.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=1.13.1->aif360[all]) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=1.13.1->aif360[all]) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=1.13.1->aif360[all]) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=1.13.1->aif360[all]) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow>=1.13.1->aif360[all]) (4.6.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=1.13.1->aif360[all]) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=1.13.1->aif360[all]) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=1.13.1->aif360[all]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=1.13.1->aif360[all]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=1.13.1->aif360[all]) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=1.13.1->aif360[all]) (3.1.1)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (5.3.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (1.7.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (5.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (2.11.3)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (0.10.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (22.1.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (0.7.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from BlackBoxAuditing->aif360[all]) (2.5.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow>=1.13.1->aif360[all]) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (2.0.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter->aif360[all]) (5.1.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->aif360[all]) (5.2.0)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.7/dist-packages (from lime->aif360[all]) (0.16.2)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime->aif360[all]) (1.1.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime->aif360[all]) (2.4.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime->aif360[all]) (7.1.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360[all]) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360[all]) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360[all]) (1.3.1)\n",
            "Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.7/dist-packages (from multiprocess->cvxpy>=1.0->aif360[all]) (0.3.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (0.5.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (0.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (3.3.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (1.4.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (21.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn==0.4.6->aif360[all]) (0.5.1)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->aif360[all]) (1.9.0)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx->aif360[all]) (1.2.4)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx->aif360[all]) (2.9.1)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.7/dist-packages (from sphinx->aif360[all]) (0.17.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx->aif360[all]) (2.1.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx->aif360[all]) (0.7.12)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx->aif360[all]) (1.2.0)\n",
            "Collecting docutils>=0.11\n",
            "  Downloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
            "\u001b[K     |████████████████████████████████| 548 kB 78.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx->aif360[all]) (1.1.5)\n",
            "Collecting shap\n",
            "  Downloading shap-0.39.0.tar.gz (356 kB)\n",
            "\u001b[K     |████████████████████████████████| 356 kB 71.9 MB/s \n",
            "\u001b[?25hCollecting memory-profiler\n",
            "  Downloading memory_profiler-0.58.0.tar.gz (36 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory-profiler->tempeh->aif360[all]) (5.4.8)\n",
            "Collecting slicer==0.0.7\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap->tempeh->aif360[all]) (1.3.0)\n",
            "Building wheels for collected packages: BlackBoxAuditing, lime, memory-profiler, shap\n",
            "  Building wheel for BlackBoxAuditing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for BlackBoxAuditing: filename=BlackBoxAuditing-0.1.54-py2.py3-none-any.whl size=1394770 sha256=3a54ad22d0ab4f138848641aa2a8438bac902cdd7f144956984b9d80c0b67086\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/9f/ee/541a74be4cf5dad17430e64d3276370ea7b6a834a76cb4215a\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283857 sha256=d785e37da3704f13e0fdc1fb681bf0c3d2709dccc8d7bf32da70fdef9c454831\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/cb/e5/ac701e12d365a08917bf4c6171c0961bc880a8181359c66aa7\n",
            "  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memory-profiler: filename=memory_profiler-0.58.0-py3-none-any.whl size=30189 sha256=bd51502050e136109c45c3277bf7ee1138a3a350a06b52f9a4954a6aeffb2114\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/19/d5/8cad06661aec65a04a0d6785b1a5ad035cb645b1772a4a0882\n",
            "  Building wheel for shap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for shap: filename=shap-0.39.0-cp37-cp37m-linux_x86_64.whl size=491646 sha256=af826045535c8726d247b7c814ab5806bac22f72f8e2d4e2fc797ff456a27d7d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/25/8f/6ae5df62c32651cd719e972e738a8aaa4a87414c4d2b14c9c0\n",
            "Successfully built BlackBoxAuditing lime memory-profiler shap\n",
            "Installing collected packages: llvmlite, slicer, numba, shap, memory-profiler, docutils, tempeh, sphinx-rtd-theme, lime, fairlearn, BlackBoxAuditing, aif360, adversarial-robustness-toolbox\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.34.0\n",
            "    Uninstalling llvmlite-0.34.0:\n",
            "      Successfully uninstalled llvmlite-0.34.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.51.2\n",
            "    Uninstalling numba-0.51.2:\n",
            "      Successfully uninstalled numba-0.51.2\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.17.1\n",
            "    Uninstalling docutils-0.17.1:\n",
            "      Successfully uninstalled docutils-0.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed BlackBoxAuditing-0.1.54 adversarial-robustness-toolbox-1.7.1 aif360-0.4.0 docutils-0.16 fairlearn-0.4.6 lime-0.2.0.1 llvmlite-0.36.0 memory-profiler-0.58.0 numba-0.53.1 shap-0.39.0 slicer-0.0.7 sphinx-rtd-theme-0.5.2 tempeh-0.1.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPEceZ5qwZeQ",
        "outputId": "076cde8d-73ab-4d96-c620-81435053849c"
      },
      "source": [
        "!pip install fairlearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fairlearn in /usr/local/lib/python3.7/dist-packages (0.4.6)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from fairlearn) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.25.1 in /usr/local/lib/python3.7/dist-packages (from fairlearn) (1.1.5)\n",
            "Requirement already satisfied: ipywidgets>=7.5.0 in /usr/local/lib/python3.7/dist-packages (from fairlearn) (7.6.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from fairlearn) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from fairlearn) (0.22.2.post1)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.0->fairlearn) (5.0.5)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.0->fairlearn) (3.5.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.0->fairlearn) (4.10.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.0->fairlearn) (5.5.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.0->fairlearn) (5.1.3)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.0->fairlearn) (1.0.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.0->fairlearn) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.0->fairlearn) (5.3.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn) (57.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn) (4.4.2)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn) (1.0.18)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5.0->fairlearn) (2.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5.0->fairlearn) (0.2.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5.0->fairlearn) (4.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.1->fairlearn) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.1->fairlearn) (2018.9)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->fairlearn) (1.0.1)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (5.3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (2.11.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (1.7.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (0.10.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5.0->fairlearn) (22.1.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (2.0.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (1.4.3)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (0.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (3.3.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (0.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (0.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (21.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (0.5.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03qSAtCgrO8P"
      },
      "source": [
        "# packes from aif360\n",
        "from aif360.datasets import StandardDataset\n",
        "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
        "from aif360.algorithms.preprocessing import LFR, Reweighing\n",
        "from aif360.algorithms.inprocessing import AdversarialDebiasing, PrejudiceRemover\n",
        "from aif360.algorithms.postprocessing import CalibratedEqOddsPostprocessing, EqOddsPostprocessing, RejectOptionClassification\n",
        "\n",
        "#more packages needed for this analysis\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvsDVm_ruUyO"
      },
      "source": [
        "## 2.1 Data cleaning and re-encoding for the AIF special format \"StandardDataset\"\n",
        "To be able to work with our dataset we transform the categorical columns into numerical columns with the label encoder and scale all truely numerical columns to [0,1] with the MinMaxScaler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "JAHJQamg4P_z",
        "outputId": "662ece93-0ae1-4818-eabf-9712c3ca37dc"
      },
      "source": [
        "data_encoded = data.copy() #make copy of data\n",
        "\n",
        "categorical_names = {}\n",
        "encoders = {}\n",
        "\n",
        "#categorical_features_index = [np.where(data.columns.values == col)[0][0] for col in columns_categorical]\n",
        "display(Markdown(\"#### Again - our categorical columns:\"))\n",
        "print(columns_categorical)\n",
        "\n",
        "# Use Label Encoder for categorical columns (including target column)\n",
        "# Label Encoder transforms categorical columns to numerical columns with a label encoding\n",
        "for feature in columns_categorical:\n",
        "    le = LabelEncoder()\n",
        "    #print(data_encoded[feature])\n",
        "    le.fit(data_encoded[feature].astype(str))\n",
        "    \n",
        "    data_encoded[feature] = le.transform(data_encoded[feature].astype(str))\n",
        "    \n",
        "    categorical_names[feature] = le.classes_\n",
        "    encoders[feature] = le\n",
        "\n",
        "numerical_features = [c for c in data.columns.values if c not in columns_categorical]\n",
        "display(Markdown(\"#### Again - our numerical columns:\"))\n",
        "print(numerical_features)\n",
        "\n",
        "\n",
        "for feature in numerical_features:\n",
        "    val = data_encoded[feature].values[:, np.newaxis]\n",
        "    mms = MinMaxScaler().fit(val)\n",
        "    data_encoded[feature] = mms.transform(val)\n",
        "    encoders[feature] = mms\n",
        "    \n",
        "data_encoded = data_encoded.astype(float)\n",
        "\n",
        "display(Markdown(\"#### Encoded Data Frame now looks like this:\"))\n",
        "data_encoded.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Again - our categorical columns:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "['Agency Name', 'Agency Type', 'City', 'State', 'Month', 'Crime Type', 'Victim Sex', 'Victim Race', 'Weapon', 'Record Source', 'Crime Solved', 'Victim Count', 'Perpetrator Count', 'Race']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Again - our numerical columns:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "['Record ID', 'Year', 'Victim Age']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Encoded Data Frame now looks like this:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Record ID</th>\n",
              "      <th>Agency Name</th>\n",
              "      <th>Agency Type</th>\n",
              "      <th>City</th>\n",
              "      <th>State</th>\n",
              "      <th>Year</th>\n",
              "      <th>Month</th>\n",
              "      <th>Crime Type</th>\n",
              "      <th>Crime Solved</th>\n",
              "      <th>Victim Sex</th>\n",
              "      <th>Victim Age</th>\n",
              "      <th>Victim Race</th>\n",
              "      <th>Weapon</th>\n",
              "      <th>Victim Count</th>\n",
              "      <th>Perpetrator Count</th>\n",
              "      <th>Record Source</th>\n",
              "      <th>Race</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>148.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.141414</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000002</td>\n",
              "      <td>148.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.434343</td>\n",
              "      <td>3.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000003</td>\n",
              "      <td>148.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.303030</td>\n",
              "      <td>2.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000005</td>\n",
              "      <td>148.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.434343</td>\n",
              "      <td>3.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000006</td>\n",
              "      <td>148.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.303030</td>\n",
              "      <td>2.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Record ID  Agency Name  Agency Type  ...  Perpetrator Count  Record Source  Race\n",
              "0   0.000000        148.0          1.0  ...                0.0            0.0   0.0\n",
              "1   0.000002        148.0          1.0  ...                0.0            0.0   1.0\n",
              "2   0.000003        148.0          1.0  ...                0.0            0.0   0.0\n",
              "3   0.000005        148.0          1.0  ...                0.0            0.0   1.0\n",
              "4   0.000006        148.0          1.0  ...                1.0            0.0   0.0\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zrs14bQE2bFw"
      },
      "source": [
        "For our first analysis we want the protected attribute to be **`Victim Race`** with the priviledged value being `White`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4s5MIro2HMZ",
        "outputId": "88e7f6f9-756d-4944-c005-2c16092c6e1a"
      },
      "source": [
        "privileged_race = np.where(categorical_names['Race'] == 'White')[0]\n",
        "privileged_race # via Label Encoder the category 'White' equals 3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkHheymXp5yB"
      },
      "source": [
        "We can now build our StandardDataset for the AIF360 tool. To do this we have to tell the dataset our dependent variable (`Crime Solved`) and which of its values corresponds to the favorable class (which is `1 = Yes`) as well as the protected attribute and its priviledged class (`Victim Race` and `White`).   \n",
        "StandardDataset will automatically remove any observations with NaN values. In this case we have the 974 observations where `Victim Age` was 998 years and thus set to NaN (see above)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aZ_DLLk8mfG",
        "outputId": "57a8af8c-fe8e-46e9-e78e-e67be1012c4f"
      },
      "source": [
        "  data_race = StandardDataset(data_encoded, \n",
        "                               label_name='Crime Solved', \n",
        "                               favorable_classes=[1], \n",
        "                               protected_attribute_names=['Race'], \n",
        "                               privileged_classes=[privileged_race])\n",
        "  \n",
        "  #tmp = data_race.convert_to_dataframe()\n",
        "  #tmp[0] # this is how you can access the pure data from the Standard Dataset. Thus tmp[0] == data_encoded\n",
        "  data_race"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Missing Data: 974 rows removed from StandardDataset.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               instance weights  features  ...                     labels\n",
              "                                           ... protected attribute       \n",
              "                                Record ID  ...                Race       \n",
              "instance names                             ...                           \n",
              "0                           1.0  0.000000  ...                 0.0    1.0\n",
              "1                           1.0  0.000002  ...                 1.0    1.0\n",
              "2                           1.0  0.000003  ...                 0.0    0.0\n",
              "3                           1.0  0.000005  ...                 1.0    1.0\n",
              "4                           1.0  0.000006  ...                 0.0    0.0\n",
              "...                         ...       ...  ...                 ...    ...\n",
              "638449                      1.0  0.999994  ...                 1.0    0.0\n",
              "638450                      1.0  0.999995  ...                 1.0    1.0\n",
              "638451                      1.0  0.999997  ...                 0.0    1.0\n",
              "638452                      1.0  0.999998  ...                 1.0    1.0\n",
              "638453                      1.0  1.000000  ...                 1.0    1.0\n",
              "\n",
              "[637480 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0bNn_Zrrgy9"
      },
      "source": [
        "Using this code snippet from the example GitHub code, we can check all the characteristics of our StandardDataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJmDqw0VG-oY"
      },
      "source": [
        "def meta_data(dataset):\n",
        "    # print out some labels, names, etc.\n",
        "    display(Markdown(\"#### Dataset shape\"))\n",
        "    print(dataset.features.shape)\n",
        "    display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
        "    print(dataset.favorable_label, dataset.unfavorable_label)\n",
        "    display(Markdown(\"#### Protected attribute names\"))\n",
        "    print(dataset.protected_attribute_names)\n",
        "    display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
        "    print(dataset.privileged_protected_attributes, dataset.unprivileged_protected_attributes)\n",
        "    display(Markdown(\"#### Dataset feature names\"))\n",
        "    print(dataset.feature_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "fly8tytbHEBh",
        "outputId": "19a8db2d-f4cf-48e9-ed6d-d7e32cac79a8"
      },
      "source": [
        "meta_data(data_race)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Dataset shape",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "(637480, 16)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Favorable and unfavorable labels",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1.0 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Protected attribute names",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "['Race']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Privileged and unprivileged protected attribute values",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[array([1.])] [array([0.])]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Dataset feature names",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "['Record ID', 'Agency Name', 'Agency Type', 'City', 'State', 'Year', 'Month', 'Crime Type', 'Victim Sex', 'Victim Age', 'Victim Race', 'Weapon', 'Victim Count', 'Perpetrator Count', 'Record Source', 'Race']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1FoRQgArt3s"
      },
      "source": [
        "To evaluate if our model is fair and \"how\" fair it is we first need to create a model. To do so we create a train and test split. We actively decided against a training-validation-test split since this project is not about finding the best model but about evaluating the fairness methods. To properly train a model you should in any case use a three-splitted dataset and at best also use cross-validation.   \n",
        "Looking at the crosstables we seem to have a good split between the training and test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "H9WujbXoHuvL",
        "outputId": "99e0c40b-70f0-4c4f-bcb8-8c053c937bb9"
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "data_race_train, data_race_test = data_race.split([0.7], shuffle=True)\n",
        "\n",
        "display(Markdown(\"#### Train Dataset shape\"))\n",
        "print(data_race_train.features.shape)\n",
        "print(pd.crosstab(data_race_train.convert_to_dataframe()[0]['Crime Solved'], data_race_train.convert_to_dataframe()[0]['Race'] == 1, normalize='index'))\n",
        "display(Markdown(\"#### Test Dataset shape\"))\n",
        "print(data_race_test.features.shape)\n",
        "print(pd.crosstab(data_race_test.convert_to_dataframe()[0]['Crime Solved'], data_race_test.convert_to_dataframe()[0]['Race'] == 1, normalize='index'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Train Dataset shape",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "(446236, 16)\n",
            "Race             False     True \n",
            "Crime Solved                    \n",
            "0.0           0.567225  0.432775\n",
            "1.0           0.475011  0.524989\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Test Dataset shape",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "(191244, 16)\n",
            "Race             False     True \n",
            "Crime Solved                    \n",
            "0.0           0.565370  0.434630\n",
            "1.0           0.475008  0.524992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s89RD6-XuLUC"
      },
      "source": [
        "## 2.2 Training a very simple random forest\n",
        "For a very naive classification we train a simple random forest (takes about 2m). As we defined as a input for our StandardDataset the `label_name = Crime Solved` the random forest will predict an outcome of `1 = Crime was solved`or `0 = crime was not solved`. After the training we evaluate the accuracy and give a confusion matrix. We know that we have an unblanced data set since the about 70% of all observations in the test set are solved crimes. Thus the accuracy of our random forest is okay-ish with about 76%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q06XLHWbH_ar"
      },
      "source": [
        "rf_race = RandomForestClassifier().fit(data_race_train.features, \n",
        "                     data_race_train.labels.ravel(), \n",
        "                     sample_weight=data_race_train.instance_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZozwVMeCI7Jk"
      },
      "source": [
        "X_test_race = data_race_test.features\n",
        "y_test_race = data_race_test.labels.ravel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "7iqRjroBJbvC",
        "outputId": "69c08bb1-51d1-492f-98dd-fa12f6253d7e"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_curve, auc\n",
        "\n",
        "def get_model_performance(X_test, y_true, y_pred, probs):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    matrix = confusion_matrix(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    preds = probs[:, 1]\n",
        "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    return accuracy, matrix, f1, fpr, tpr, roc_auc\n",
        "\n",
        "accuracy, confusion_matrix, f1, fpr, tpr, roc_auc = get_model_performance(X_test_race, y_test_race, rf_race.predict(X_test_race), rf_race.predict_proba(X_test_race))\n",
        "\n",
        "display(Markdown(\"#### Percent of Crime Solved = 1\"))\n",
        "print(np.mean(y_test_race))\n",
        "display(Markdown(\"#### Accuracy on Test Set\"))\n",
        "print(accuracy)\n",
        "display(Markdown(\"#### Confusion Matrix\"))\n",
        "print(confusion_matrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Percent of Crime Solved = 1",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.702479554914141\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Accuracy on Test Set",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.757618539666604\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Confusion Matrix",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[[ 26775  30124]\n",
            " [ 16230 118115]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Omswj0nDQ2ZB"
      },
      "source": [
        "# 3. Bias Detection <a class=\"anchor\" id=\"chapter3\"></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u9sbrrNdLZn"
      },
      "source": [
        "If bias is to be mitigated in a data set or model, it must first be detected. For this purpose, we will explain fairness definitions and metrics we consider important in this section. Afterwards we are going to apply them to the random forest model under consideration and evaluate whether and if so, to which extent it is biased.\\\n",
        "There are over 20 different definitions of fairness in literature, some of which overlap and contradict each other. The definitions can be roughly divided into Group and Individual Fairness. \n",
        "In Group Fairness different groups should be treated euqally, whereas in Individual Fairness similar individuals should reiceive similar predictions (Mehrabi et al. 2019). Here, the group fairness definitions in particular are taken into account. All existing fairness metrics are based on fairness definitions. The metrics presented here form the basis for the bias mitigation algorithms considered later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddekPdeb5Npx"
      },
      "source": [
        "For reusability, we implemented a method that computes a given fairness metric on a given model and dataset. Therefore we determine the privileged and unprivileged group of the protected attribute `Victim Race`. The privileged group is assigned to all victims being white and the unprivileged group is assigned to all remaining categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXX7Gjmt5CKV"
      },
      "source": [
        "privileged_groups = [{'Race': 1}]\n",
        "unprivileged_groups = [{'Race': 0}] # careful: some function can not handle more than 1 unpriviledged group -> you will have to convert these into a binary group beforehand"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27qaY0pD5HU7"
      },
      "source": [
        "def calc_fairness(ori_dataset, predicted_values, privileged_groups, unprivileged_groups, fair_metric, printout = True):\n",
        "  # for predicted_values run: model.predict(ori_dataset.features)\n",
        "  pred_dataset = ori_dataset.convert_to_dataframe()[0]\n",
        "  pred_dataset[ori_dataset.label_names[0]] = predicted_values\n",
        "  pred_dataset = StandardDataset(pred_dataset, \n",
        "                               label_name=ori_dataset.label_names[0], \n",
        "                               favorable_classes=[1], \n",
        "                               protected_attribute_names=['Race'], \n",
        "                               privileged_classes=[privileged_race])\n",
        "\n",
        "          \n",
        "  classified_metric = ClassificationMetric(ori_dataset,\n",
        "                                              pred_dataset,\n",
        "                                              unprivileged_groups=unprivileged_groups,\n",
        "                                              privileged_groups=privileged_groups)\n",
        "\n",
        "  metric_pred = BinaryLabelDatasetMetric(ori_dataset,\n",
        "                                              unprivileged_groups=unprivileged_groups,\n",
        "                                              privileged_groups=privileged_groups)\n",
        "\n",
        "  row = calc_fairness_single(classified_metric, metric_pred, fair_metric)\n",
        "  acc = classified_metric.accuracy()\n",
        "  if printout:\n",
        "    display(Markdown(\"#### Fairness metric:\"))\n",
        "    print(fair_metric,\": \", row)\n",
        "    display(Markdown(\"#### Accuracy:\"))\n",
        "    print(acc)\n",
        "  else:\n",
        "    return row, acc\n",
        "\n",
        "\n",
        "def calc_fairness_single(classified_metric, metric_pred, fair_metric):\n",
        "  #should be 0\n",
        "  if fair_metric == \"equal opportunity\":\n",
        "    return classified_metric.equal_opportunity_difference()\n",
        "  #should be 1, <1 prefers privileged, >1 prefers unprivileged\n",
        "  elif fair_metric == \"disparate impact\":\n",
        "    return metric_pred.disparate_impact()\n",
        "  #should be 0\n",
        "  elif fair_metric == \"equalized odds\":\n",
        "    return classified_metric.average_odds_difference()\n",
        "  #should be 0\n",
        "  elif fair_metric == \"statistical parity\":\n",
        "    return metric_pred.mean_difference()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-h9gjZ7qXJL"
      },
      "source": [
        "All equations of our 4 fairness metrics are from the documentation of the AIF360, available via \\\\\n",
        "https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.ClassificationMetric.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4qfJqAQj9_5"
      },
      "source": [
        "## 3.1 Statistical Parity / Demographical Parity\n",
        "\n",
        "\"The likelihood of a positive outcome should be the same regardless of whether the person is in the protected group.\" (Mehrabi et al 2019)\n",
        "\n",
        "**Definition**: (Verma & Rubin 2018) \\\\\n",
        "\\begin{equation}\n",
        "P(d = 1 | G = privileged) = P(d = 1 | G = unprivileged) \n",
        "\\end{equation}\n",
        "\n",
        "In other words, the sum of True Positves and False Postives should be the same for all groups of the protected attribute.\n",
        "\n",
        "**Metric for Statistical Parity**: \n",
        "\\begin{equation}\n",
        "P(d = 1 | G = unprivileged) - P(d = 1 | G = privileged)\n",
        "\\end{equation}\n",
        "\n",
        "In order for a dataset or model to be fair, the statistical parity should be 0.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olpx1nlM7lw3"
      },
      "source": [
        "Let's have a look at Statistical Parity Metric for our random forest model trained with the Homicide dataset. First we have to generate the predictions of the random forest, so that they can be fed into the method calc_fairness implemented above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "kvj4Rk7g48AB",
        "outputId": "7777b778-89b9-42f5-b642-3b83cefc0da3"
      },
      "source": [
        "prediction = rf_race.predict(data_race_test.features)\n",
        "calc_fairness(data_race_test, prediction, privileged_groups, unprivileged_groups, \"statistical parity\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Fairness metric:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "statistical parity :  -0.07554431748492474\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Accuracy:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.757618539666604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vki3ltYu8kds"
      },
      "source": [
        "The calculated value for statistical parity is -0.075 and indicates that the model is slightly biased. Non-surprisingly it prefers the privileged group (white group). This would mean that more crimes were solved of where victims were white. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SFucVrKXr5t"
      },
      "source": [
        "##3.2 Equalized odds\n",
        "\n",
        "\"[...] The probability of a person in the positive class being correctly assigned a positive outcome and the probability of a person in a negative class being incorrectly assigned a positive outcome should both be the same for the protected and unprotected group members.\" (Mehrabi et al. 2019)\n",
        "\n",
        "**Definition**: (Verma & Rubin 2018) \\\\\n",
        "\n",
        "\\begin{equation}\n",
        "P(d = 1|Y = 1, G = privileged) = P(d = 1|Y = 1, G = unprivileged)\\\\\n",
        "which\\ also\\ holds\\ for\\ Y = 0.\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "In other words, protected and unprotected groups should have the same True-Postive and False-Positive Rates. \n",
        "\n",
        "**Metric for Equalized Odds**:\n",
        "\\begin{equation}\n",
        "\\frac{1}{2}[(FPR_{D=unprivileged}−FPR_{D=privileged})+(TPR_{D=unprivileged}−TPR_{D=privileged}))]\n",
        "\\end{equation}\n",
        "\n",
        "In order for a dataset or model to be fair, equalized odds should be 0.\n",
        "\n",
        "\n",
        "Reminder: \\\n",
        "\n",
        "\\begin{equation}\n",
        "TPR = \\frac{TP}{TP+FN}\n",
        "\\end{equation}\n",
        "\\\n",
        "\\begin{equation}\n",
        "FPR = \\frac{FP}{FP+TN}\n",
        "\\end{equation}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5d9R-hU7iGP"
      },
      "source": [
        "Let's have a look at the Equalized Odds Metric for our random forest model trained with the Homicide dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "URjONxGM5Xzp",
        "outputId": "ae0878b6-b5f0-4223-ff2d-375b7ef1228b"
      },
      "source": [
        "calc_fairness(data_race_test, prediction, privileged_groups, unprivileged_groups, \"equalized odds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Fairness metric:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "equalized odds :  -0.10529814146290331\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Accuracy:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.757618539666604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWp9llLH7gew"
      },
      "source": [
        "The calculated value for Equalized Odds is -0.105 which indicates that the model is biased because it differs from 0. Compared to Statistical Parity, equalized odds shows a stronger bias in the model. Both metrics indicate that the model favors the white group. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH1xPVjqbbYY"
      },
      "source": [
        "##3.3 Equal Opportunity\n",
        "\n",
        "\"[...] The probability of a person\n",
        "in a positive class being assigned to a positive outcome should be equal for both protected and\n",
        "unprotected group members\" (Mehrabi et al. 2019)\n",
        "\n",
        "**Definition**: (Verma & Rubin 2018) \\\\\n",
        "\\begin{equation}\n",
        "TPR_{D=unprivileged} = TPR_{D=privileged}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "In other words, same True Postive Rates for all groups of the protected attribute.\n",
        "\n",
        "**Metric for Equal Opportunity**: \n",
        "\\begin{equation}\n",
        "TPR_{D=unprivileged} - TPR_{D=privileged}\n",
        "\\end{equation}\n",
        "\n",
        "In order for a dataset or model to be fair, equal opportunity should be 0.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIsfOlhIAP-W"
      },
      "source": [
        "Let's have a look at the Equal Opportunity metric for our random forest model trained with the Homicide dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "x2mA7W3Q5eJ1",
        "outputId": "42e416ec-74b7-4357-b0e3-92da6f3077ca"
      },
      "source": [
        "calc_fairness(data_race_test, prediction, privileged_groups, unprivileged_groups, \"equal opportunity\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Fairness metric:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "equal opportunity :  -0.06753600044098129\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Accuracy:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.757618539666604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZqGbIKtAQno"
      },
      "source": [
        "Since the value -0.068 differs from 0, also Equal Opportunity shows that the model is slightly biased. Not as strong as Equalized Odds, but similar to Statistical parity. Similar to Statistical Parity and Equalized Odds it indicates that the privileged group (white group) is favored by the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTmzCUCTcp2Y"
      },
      "source": [
        "##3.4 Disparate Impact \n",
        "\n",
        "For Disparate Impact the ratio of all instances in the positive/favorable class with respect to membership of the protected or unprotected group is calculated. \n",
        "\\begin{equation}\n",
        "\\frac{P(Y=1|D=unprivileged)}{P(Y=1|D=privileged)}\n",
        "\\end{equation}\n",
        "\n",
        "In order for a dataset or model to be fair, Disparate Impact should be 1.\n",
        "A Disparate Impact value smaller than 1 indicates that the privileged group is favored by the model. Similarly, a Disparate Impact value greater than 1 indicates that the unprivileged group is favored."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9IFDhOZAR6a"
      },
      "source": [
        "Let's have a look at the Disparate Impact in our random forest model trained with the Homicide dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "Z-gn2UyI5lW4",
        "outputId": "3428967e-93ae-4eac-f0e6-4cbe788c8874"
      },
      "source": [
        "calc_fairness(data_race_test, prediction, privileged_groups, unprivileged_groups, \"disparate impact\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Fairness metric:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "disparate impact :  0.8979675076759687\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Accuracy:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.757618539666604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lutkYy3WAS8I"
      },
      "source": [
        "The calculated value for Disparate Impact is 0.898 which is smaller than 1. Therefore the model favors the privileged group (white group) similar to previous metrics under consideration. Feldman et al. 2015 state that a DI value above 0.8 is considered to be fair because it is advocated by the US Equal Employment Opportunity Commission (EEOC). A different value might be considered acceptable from a different organization in Europe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-TxkLSJRHfE"
      },
      "source": [
        "#4. Bias Mitigation <a class=\"anchor\" id=\"chapter4\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k263KOmrRcHz"
      },
      "source": [
        "Following the investigation by Bellamy et al., we can now try to mitigate the detected bias in our model. To do this, we apply selected bias mitigation algorithms of the  AIF360 toolkit to our random forest. The nine algorithms of the tool are divided into pre-, in- and post-processing techniques and are based on previous research. An overview is given in the following table which is closely related to the work of Bellamy et al. We will apply four selected algorithms here. All bias mitigation algorithms were designed to attempt to satisfy one or more fairness metrics. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKu74-oYl4SW"
      },
      "source": [
        "| | Application | Bias Mitigation Algorithm | Source |\n",
        "| --- | --- | --- | ---|\n",
        "| **Pre-Processing** | If it's possible to modify training data |  1. Reweighing | (Kamiran \\& Calders 2012)|\n",
        "| | | 2. Optimized Pre-processing | (Calmon et al. 2017)|\n",
        "| | | 3. Learning Fair Representations |  (Zemel et al. 2013)|\n",
        "| | | 4. Disparate Impact Remover | (Feldman et al. 2015) |\n",
        "| **In-Processing** | If it's possible to change algorithm during training | 5. Adversarial Debiasing | (Zhang et al. 2018) |\n",
        "| | | 6. Prejudice Remover | (Kamishima et al. 2012) |\n",
        "| **Post-Processing** | Last chance, if algorithm is a black box | 7. Reject Option Classification | (Kamiran et al. 2012)\n",
        "| | | 8. Equalized Odds Post-processing | (Hardt et al. 2016)|\n",
        "| | | 9. Calibrated Equalized Odds Post-processing | (Pleiss et al. 2017)|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi44Lkpp6U4V"
      },
      "source": [
        "## 4.1 Pre-processing \n",
        "\n",
        "Pre-processing is done before training a model. Here, only the dataset that will later be fed into a model is modified.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7-dTuRMLfW-"
      },
      "source": [
        "###1. Reweighing\n",
        "\n",
        "In Reweighing (Kamiran & Calders, 2012) each training example is assigned to a weight before training. The weights are based upon their category of the protected attribute and target output (meaning the actual output, not the predicted one, as we don't need our random forest here). The weights are adjusted such that the discrimination is reduced to zero while the overall positive class probability maintains the same. \n",
        "The accuracy of our Random Forest stays the same before and after application, because Reweighing only modifies the data set. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMYS33rZbGRw"
      },
      "source": [
        "Our data set was already provided with initial instance weights of 1.0 when creating the StandardDataset of AIF360. In this section we want to adjust these weights by applying Reweighing to the data so that we can compensate for the bias in the data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKF6ADwfbG_F",
        "outputId": "622dbf32-a3b5-4d91-b559-1b3eed618fe5"
      },
      "source": [
        "data_race_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               instance weights  features  ...                     labels\n",
              "                                           ... protected attribute       \n",
              "                                Record ID  ...                Race       \n",
              "instance names                             ...                           \n",
              "401732                      1.0  0.629227  ...                 0.0    1.0\n",
              "248795                      1.0  0.389684  ...                 1.0    1.0\n",
              "619460                      1.0  0.970252  ...                 0.0    1.0\n",
              "173469                      1.0  0.271702  ...                 0.0    1.0\n",
              "87124                       1.0  0.136461  ...                 1.0    0.0\n",
              "...                         ...       ...  ...                 ...    ...\n",
              "44166                       1.0  0.069177  ...                 0.0    1.0\n",
              "202125                      1.0  0.316586  ...                 1.0    1.0\n",
              "448572                      1.0  0.702592  ...                 0.0    0.0\n",
              "301716                      1.0  0.472574  ...                 0.0    0.0\n",
              "98904                       1.0  0.154912  ...                 0.0    0.0\n",
              "\n",
              "[446236 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUlqa7H-XHh2"
      },
      "source": [
        "In order to ultimately determine whether the application of the Reweighing algorithm had an effect, we first need a metric that measures the bias prior to our application in the Homicide dataset. In this case, Statistical Parity is used, because Reweighing is based on this metric due to the fact that it evalutes the protected attribute and target class. Let's have a look!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "TDqY5swGW1zE",
        "outputId": "c9050692-72ec-4d13-aa1a-b67111d4e769"
      },
      "source": [
        "# Metric for the original dataset\n",
        "metric_orig_train = BinaryLabelDatasetMetric(data_race_train,\n",
        "                                             unprivileged_groups=unprivileged_groups,\n",
        "                                             privileged_groups=privileged_groups)\n",
        "display(Markdown(\"#### Original training dataset\"))\n",
        "print(\"Difference in statistical parity outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Original training dataset",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Difference in statistical parity outcomes between unprivileged and privileged groups = -0.077098\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvW76-IRXoFc"
      },
      "source": [
        "According to the authors of Reweighing, a data set is unbiased if the protected attribute and the target class are statistically independent (Kamiran & Calders 2012). This is not the case here. As mentioned in the bias detection section, we can see that our dataset is not completely free of bias, as the Statistical Parity slightly differs from 0. A Statistical Parity of -0.077 means that the privileged group is favored by our dataset.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywfUToTva0dD"
      },
      "source": [
        "Let us apply the Reweighing algorithm so that we can compensate for the measured bias. To do this, we first call the constructor to create a Reweighing object. Then we fit it with our original training set \"data_race_train\" and transform it by assigning a weight to each example.  Each weight is composed of membership in the privileged or unprivileged group and membership in the favored or unfavored class. In this case: Weights are assigned with respect to being white (privileged) or PoC (unprivileged) and whether the crime was solved (favorable) or not (unfavorable)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1zwIeg7Uua3"
      },
      "source": [
        "reweighing = Reweighing(unprivileged_groups=unprivileged_groups,\n",
        "               privileged_groups=privileged_groups)\n",
        "reweighing.fit(data_race_train)\n",
        "data_train_reweighed = reweighing.transform(data_race_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nwbjqrf5WSgz"
      },
      "source": [
        "### Testing whether weighting worked correctly\n",
        "assert np.abs(data_train_reweighed.instance_weights.sum()-data_race_train.instance_weights.sum())<1e-6\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvaFsMfPgNS8"
      },
      "source": [
        "As we now applied Reweighing to out training set, let's check whether it is free of bias by calculating the Statistical Parity of the transformed data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "B_qq9fB7Wb06",
        "outputId": "e3805008-7fe9-40fb-9001-d103e54d756c"
      },
      "source": [
        "metric_transf_train = BinaryLabelDatasetMetric(data_train_reweighed, \n",
        "                                         unprivileged_groups=unprivileged_groups,\n",
        "                                         privileged_groups=privileged_groups)\n",
        "display(Markdown(\"#### Transformed training dataset\"))\n",
        "print(\"Difference in statistical parity outcomes between unprivileged and privileged groups = %f\" % metric_transf_train.mean_difference())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Transformed training dataset",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Difference in statistical parity outcomes between unprivileged and privileged groups = -0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlZHixf3gGHe"
      },
      "source": [
        "As can be seen from the new statistical parity value of 0, Reweighing successfully eliminated the bias from our dataset by adjusting the weights of each observation accordingly. \n",
        "Our protected attribute `Race` and the class `Crime Solved` are no longer statiscally dependent. Thus, our dataset no longer favors the privileged (white) group. Let's see how the instance weights have changed after the procedure. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4083A-dZ6Hm",
        "outputId": "f372729a-04d9-41ec-cd75-edc4305150ba"
      },
      "source": [
        "data_train_reweighed"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               instance weights  features  ...                     labels\n",
              "                                           ... protected attribute       \n",
              "                                Record ID  ...                Race       \n",
              "instance names                             ...                           \n",
              "401732                 1.057764  0.629227  ...                 0.0    1.0\n",
              "248795                 0.947735  0.389684  ...                 1.0    1.0\n",
              "619460                 1.057764  0.970252  ...                 0.0    1.0\n",
              "173469                 1.057764  0.271702  ...                 0.0    1.0\n",
              "87124                  1.149675  0.136461  ...                 1.0    0.0\n",
              "...                         ...       ...  ...                 ...    ...\n",
              "44166                  1.057764  0.069177  ...                 0.0    1.0\n",
              "202125                 0.947735  0.316586  ...                 1.0    1.0\n",
              "448572                 0.885803  0.702592  ...                 0.0    0.0\n",
              "301716                 0.885803  0.472574  ...                 0.0    0.0\n",
              "98904                  0.885803  0.154912  ...                 0.0    0.0\n",
              "\n",
              "[446236 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ADyCTJVZ4ig"
      },
      "source": [
        "We can see that the weights no longer have initial values 1.0. We adjusted them successfully according to Statistical Parity, so that the data set is no longer biased. The transformed dataset could now be used to train any classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uo7rnlE0RqNF"
      },
      "source": [
        "###4. Disparate Impact Remover\n",
        "\n",
        "The Disparate Impact Remover (Feldman et al. 2015) (DIR) aims to optimize the already mentioned Disparate Impact Fairness Metric (see the paragraph for bias detection) in a dataset. To achieve this it takes into account the distribution of all unprotected attributes (Y) given the values of the protected attributes (X). In our case this would be `x = White` and `x = PoC`. \u0010When we have bias in our data we have a different cummulative distribution functions (cdf)\n",
        "\\begin{equation}\n",
        " F_{White}(Y=y)\\ and\\ F_{PoC}(Y=y)\n",
        "\\end{equation}\n",
        "DIR now changes the values of all y in Y so that the cdfs match whilst preserving the rank of each observation. This means if the age of a white victim was 34 and thus had the rank of let's say 124 it might now be assigned the age of 23 whilst keeping it's rank of 124 in the white group. Same happens for the PoC group. \n",
        "<br />  \n",
        "To control for the matching of the cdfs a parameter called `repair level` is used. For completely matching cdfs the repair level is equal to 1. If we still want to allow for slight differences in the cdfs of the repective groups we can lower the repair level. A reason to do this might be to keep some information about the groups to train our model on. On the other hand we shouldn't keep to much information since we are trying to mitigate bias. A repair level of 0 is the same as if we didn't use DIR at all.   \n",
        "<br />\n",
        "A very nice visualisation of how the repair level works can be found here:    \n",
        "https://towardsdatascience.com/ai-fairness-explanation-of-disparate-impact-remover-ce0da59451f1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNe-zx1xgTYy"
      },
      "source": [
        "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
        "from aif360.metrics import BinaryLabelDatasetMetric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iugBK7dgoJW"
      },
      "source": [
        "protected = 'Race'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c343g5AFyJ9r"
      },
      "source": [
        "The Disparate Impact Remover takes about an hour with the whole dataset to run through the process. Since we want to find the optimal parameter for the repair level (in [0,1]) we need to run a loop for different values of the repair level. This would result in an one hour run per loop.   \n",
        "For demonstration purpose we generate a small random sample from the whole dataset on which we run the Disparate Impact Remover.   \n",
        "Results for the whole dataset can be found below as a picture. If you want to regenerate this picture please run the code below with `whole_dataset = True`.   \n",
        "`num_points` defines the number of values for the repair level to be visited and later drawn in the diagram. It's currently set to 6 (steps of 0.2) to reduce runtime. However feel free to experiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZ1n8uQ8hcYO"
      },
      "source": [
        "whole_dataset = False\n",
        "num_points = 6\n",
        "\n",
        "if whole_dataset:\n",
        "  data_DIR_train = data_race_train.copy()\n",
        "  data_DIR_test = data_race_test.copy()\n",
        "else:\n",
        "  np.random.seed(3456)\n",
        "  d_train_min, _ = data_race_train.split([0.05], shuffle=True)\n",
        "  d_test_min, _ = data_race_test.split([0.05], shuffle=True)\n",
        "  data_DIR_train = d_train_min\n",
        "  data_DIR_test = d_test_min"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnr2qcRd795T"
      },
      "source": [
        "For every repair level we first apply the DIR (line 7-9) to our StandardDataset, both for the training and test set. Then we split our training and test sets into our features (X) and our classification variable (y) (line 11-14) before training a new Random Forest on the training set (line 20). Last we predict the new classification on the test set (line 22-23) and calculate the Disparate Impact and accuracy (line 25-27)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3r_7eKmhCbr",
        "outputId": "1cae73c8-3caa-4cc8-f8f3-f4a5a27c8e66"
      },
      "source": [
        "index = data_race_train.feature_names.index(protected) # location of column of the protected attribute\n",
        "\n",
        "DIs = []\n",
        "ACCs = []\n",
        "\n",
        "for level in tqdm(np.linspace(0., 1., num_points)): # every 0.2\n",
        "    dir = DisparateImpactRemover(repair_level=level)\n",
        "    train_trans_DIR = dir.fit_transform(data_DIR_train) \n",
        "    test_trans_DIR = dir.fit_transform(data_DIR_test) \n",
        "    \n",
        "    X_tr = np.delete(train_trans_DIR.features, index, axis=1)\n",
        "    X_te = np.delete(test_trans_DIR.features, index, axis=1)\n",
        "    y_tr = train_trans_DIR.labels.ravel()\n",
        "    y_te = test_trans_DIR.labels.ravel()\n",
        "    \n",
        "    # if you want a logistic regression use this here:\n",
        "    #lmod = LogisticRegression(class_weight='balanced', solver='liblinear')\n",
        "    #lmod.fit(X_tr, y_tr)\n",
        "\n",
        "    rf_mod = RandomForestClassifier().fit(X_tr, y_tr)\n",
        "    \n",
        "    test_trans_pred = test_trans_DIR.copy()\n",
        "    test_trans_pred.labels = rf_mod.predict(X_te)\n",
        "\n",
        "    calcmetric = BinaryLabelDatasetMetric(test_trans_pred, privileged_groups=privileged_groups, unprivileged_groups=unprivileged_groups)\n",
        "    DIs.append(calcmetric.disparate_impact())\n",
        "    ACCs.append(accuracy_score(y_te, test_trans_pred.labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6/6 [00:43<00:00,  7.23s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJZOfx0v9ciA"
      },
      "source": [
        "We now plot the Disparate Impact (DI) (blue line) and accuracy (yellow line). The green and red line show the preferred bounds for DI with the red line being 0.8 and the green line being at 1.0. Thus any DI above the green line is an overshooting of the DIR algorithm, now favouring the unpriviledged group. Everything between the red and the green line is considered to be an acceptable value for the DI according to Feldman et al. 2015."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "h1j61yOtlwaE",
        "outputId": "e1f20bb1-b40e-4674-ca0c-157c8d3b5a28"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "plt.plot(np.linspace(0, 1, num_points), DIs, marker='o')\n",
        "plt.plot(np.linspace(0, 1, num_points), ACCs, marker='x')\n",
        "plt.plot([0, 1], [1, 1], 'g')\n",
        "plt.plot([0, 1], [0.8, 0.8], 'r')\n",
        "plt.ylim([0.4, 1.2])\n",
        "plt.ylabel('Disparate Impact (DI)')\n",
        "plt.xlabel('repair level')\n",
        "plt.show()\n",
        "\n",
        "print( DIs)\n",
        "print(ACCs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEHCAYAAACtAv3IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZwU1b338U93z8IIIsQNAQ2o+AuuEYzBKGrEhGhiokaT3JCoUfPcJPg8mmswbsnVm8XkMTwomsXcG68xcUlcQI0aucE9qEFERMSfCeDGgKCETYZZuvv5o6pnepqZnmbomp6Z+r5fr35N1TnVVb9iht85fep0VSKbzSIiIv1fstIBiIhIz1DCFxGJCSV8EZGYUMIXEYkJJXwRkZhQwhcRiYmqKHduZgcD9wEz3P3GgrqPA9cAacCB8909E2U8IiJxFlkP38wGAjcAczvZ5NfAGe5+NLAz8KmoYhERkWiHdBqBk4H6TurHu/vb4fJaYNcIYxERib3IhnTcvQVoMbPO6jcCmNlewCeB7+XXm1kt8BFgFcGwj4iIdC0F7AXMd/fG/IpIx/C7YmZ7AA8A33L39wqqPwI81fNRiYj0CxOBp/MLKpbwzWww8DBwhbvP6WCTVQC33XYbw4YN69HYRET6qtWrVzNlyhQIc2i+SvbwpxPM3vlzJ/VpgGHDhjFy5Miei0pEpH/YZig8soRvZuMJkvoooNnMzgDuB1YAjwBnAWPM7PzwLbe7+6+jikdEJO6ivGi7ADi+yCa1UR1bRES2pW/aiojEhBK+iEhMKOGLiMSEEr6ISEwo4YuIxIQSvohITCjhi4jEhBK+iEhMKOGLiMSEEr6ISEwo4YuIxIQSvohITCjhi4jEhBK+iEhMKOGLiMSEEr6ISEwo4YuIxIQSvohITET6EHMzOxi4j+Bh5TcW1A0AbgIOcvcjooxDREQi7OGb2UDgBmBuJ5tcC7wY1fFFRKS9KId0GoGTgfpO6i8HZkV4fBERyRNZwnf3FndvKFK/Kapji4jItnTRVkQkJpTwRURiQglfRCQmIpuWaWbjgenAKKDZzM4A7gdWuPssM7sL2DvY1B4Hfu3ut0cVj4hI3EWW8N19AXB8kfozozq2iIhsS0M6IiIxoYQvIhITSvgiIjGhhC8iEhNK+CIiMaGELyISE0r4IiIxoYQvIhITSvgiIjGhhC8iEhORPuJQRKS7Zi9cybWPOPXrGxg+pI5pk41TDx9R6bD6NCV8Eel1Zi9cyWX3LqahOQ3AyvUNXHbvYgAl/R2ghC8iPSabzdKczrK1Jc3W5jRbmzKtyw1Naba2ZNjanObqB5a0JvuchuY0Vz2whEQCqlNJqlNJaqqSVKcS1FblryepyV8Ot6lJJUkkEhU689JE/alGCV+kj4gqGWQyWRrDRNvQHCbi5gwNzWkam9NsbUnT0BTUB8vptu2b0mHCzts+XN6at6/W5ZYM6Uy227Gu39LMhXe+2O33V6cSnTQMiW3KasJGpboqV5booCy3XYKaqlTrfmpSBQ1QrixvH7lGKrevB1+q5/JZL0f6qUYJX6QPCIY4XqKhOQMEyWDa3Yt4dvl7HDh8cJB4mzN5CTlMvK0JOU1DcyZMyLnEHiTixpZMt2JKJmBAdYoB1SnqqlPUVicZUJWiribFgOokQ+qqGVCTYkBVsF4XbjugOtn6vtx6Yd3X/ns+azY1bnPMPQfXcsfXJ9CcztLUkqEpnaGpJUNz/s/W5SxNLengZ6fbZTt87+bGFprTGZpb2t7blG7//uZ09xuuUjU0p7n2EVfCF4mTnzz8amuyz2lOZ7lz/lvtympSSWoLEmiQjFPsUldN3eDaoDxMzO2SdFWQbOtqUtRWtSXfjvY1oDoZ6RDJ5SePbTeGD1BXneKyk8ay7+6DIjnm9spms2EjkG3XmBQ2DI0t7bdpK2vfCP1szmsdHqd+fUPZYlbCF+nlHlhUz+qNWzusSwDPX3lia5JOJXv3GHWpcj3a3jxLJ5FIUFuVorYKqN3x/d3xt7dY2UFyHz6kbsd3HlLCF+ml1r3fxPdmv8yDi1dRnUp0OIQwfEgduw4qQ7bphU49fESvSvBRmzbZOvxUM22yle0YSvgivdAjS1ZzxazFbGxo4ZJPGcN2ruWK2UsiTQZSWT3xqSbShG9mBwP3ATPc/caCuhOBHwNp4CF3/0GUsYj0BRu2NHP1A0u4d+FKDho+mNvO/zA2bGcAkslkrx7ikB0X9aeayBK+mQ0EbgDmdrLJTGAysBJ4wszucfdXoopHpLd73Nfw3Xte4r3NTVw4aQwXnLA/1am2u5/EbYhDyi/KHn4jcDLw3cIKM9sXWOfub4XrDwGTgLIk/FsX3crNC28ux65EIpfOZHnjvS2s2bSVnWpS7PfBQdy3sor7flfpyKRSzj38XM467Kyy7zeyhO/uLUCLWYdjjMOAtXnra4D9oopFpLfa2NDMsrXv05ROM3xIHSOH1pHs5d8Glb6rt1y0Letf+FmHnRVJ6yhSLluaWvjJw69y66tvMH63gfzsC4cxbp+hlQ5L+rlKJfx6gl5+zoiwTKTfe/71dVx81yLeXLeFc48ezbTJRl1NqtJhSQxUJOG7++tmNtjMRgFvA58BplQiFpGesrU5zfQ5zn89vYKRQ+u48+sT+Oi+u1Y6LImRKGfpjAemA6OAZjM7A7gfWOHus4BvAneEm//B3Tv+XrFIP/DiW+u5+I8vsmzt+3xlwj5cdtJYBtb2lhFViYsoL9ouAI4vUv8kcFRUxxfpDRpb0syc+3d+9cRy9ty5lt+ddyQTx+xe6bAkptTFEInIkvoNXPzHRby6ehNnjh/J9045kMEDqisdlsSYEr5ImTWnM/zy8WXMnPt3hg6s4TdnH8GksXtWOiyR0hK+me0H7B2uvu3u/4guJJG+67V3NnHxHxexeOUGPvfh4Vx1ykEMHVhT6bBEgCIJ38xSwEXAN4B3CG6BADDCzPYEbgKuC79gJRJr6UyW/3xqOf9vzmsMGlDFL6eM46RD9qp0WCLtFOvhPxS+DnP3LfkVZlYHfB14kOB+OCKxtXztZr5z1yJeeHM9nzpoGD887WB266e3LJa+rVjCP9fdV3ZU4e4NwEwzuyeasER6v0wmy2+feZ2f/vlVaqtSXP+lD/PZw4b3+gdlS3wVS/gXdnIfHADc/ZLOGgSR/u6tdVvCZ8qu44QP7cE1px/CnoMHVDoskaKKJfwlPRaFSB+RzWa5/W9v8uMHl5JIJPi/ZxzKmeNHqlcvfUKnCd/dfwtgZsOAsQQPKnnZ3df1UGwivUr9+ga+e89LPPX3dzlm/9346RmHMqKMzxsViVqxWToDgP8GDgMWAoOBA8N7109z946fqizSz2SzWe5e8Db/8adXSGey/ODUg/nKR/dRr176nGJDOj8heCDJl909C2BmVcBVwHUE0zVF+rU1m7Zy+b2L+cvSNRw56gNce+ahfHDXgZUOS6RbiiX88e4+Mb8gnHN/pZm9GG1YIpX3wKJ6vnffyzQ0pbny02M59+jRJJPq1UvfVSzhNxep0zi+9FvvbW7k+/ct4cHFq/jw3kOY/oXD2G/3QZUOS2SHFUv4u5nZyR2UJwDdxFv6pUeWrOaKWYvZ2NDCJZ8y/tfEfanKe5C4SF9WLOEvAM7spO6FCGIRqZgNW5q56oElzFq4koOGD+a28z+MDdu50mGJlFWxhP8f7r6i2JvNbJS7v17ekER61mO+hkvveYn3Njdx4aQxXHDC/lSrVy/9ULGEf6OZPQn8yt035FeY2S7AvwLHEjyeUKTP2bS1mR/+aSl/eP4tDthzEL85+yMcPGKXSoclEpliCf8UgscQPmdmWwmePZsluE3yAOBG4HORRygSgb/+410uufslVm1o4JvH78dFJ46htkoPEpf+rdg3bTPAz4Gfm9newPCwqt7d3ypl52Y2A5hA0FBc6O7z8+o+B1wJNAJ3uvuN3TsFkdJtaWrhJw+/yq3PvMG+uw3k7m9+jHH7DK10WCI9oqQHoIQJvqQkn2NmxwFj3P0oMxsL3Ez4DFszSxJ8QhgHvAc8bGaz3f3t7TmGyPaY//o6vnPXIt5ct4Vzjx7NtMlGXY169RIfUV6ZmgTMBnD3pcBQMxsc1u0GrHf3teEnibnAiRHGIjG2tTnND//0Cl+46Rky2Sx3fn0C3z/lQCV7iZ0uE76ZjeygbGwJ+x4GrM1bXxuW5ZZ3NrMxZlYNfBzQQz+l7F58az2fnvkU//X0CqZ8dB/+fOGxfHRffY1E4qnYzdN2I0jCN5vZOQRfuAKoBu4CDtjOY7V+J93ds2Z2NsEwzwZgRX69yI5qbEkzc+7f+eXjyxg2eAC/O+9IJo7ZvdJhiVRUsTH8scC5BIn9F3nlGeD3Jey7nrYePQQXfVflVtz9CWAigJldA7xeUsQiXVhSv4GL/7iIV1dv4szxI/neKQcyeEB1pcMSqbhis3SeAp4ys9uAp9y9EYI5+IXz8jsxB7gauMnMxhHM7tmUqzSzh4GzgfcJpoBO7/5piEBzOsMvHlvGDY/+naEDa/jN2UcwaaxGCkVySrloexDBEE7O783s/3T1JnefBywws3nATGCqmZ1jZqeFm/wnQaPwNHCNu7+7faGLtHntnU2c/ot5zPjLa3z60L2Yc9GxSvYiBUqZlvlF4Ji89c8SJOmZXb3R3S8tKFqUV3cvcG8Jx98usxeu5NpHnPr1DQwfUse0ycaph48o92GkwvJ/zzsPqOL9xhZ22amGX04Zx0mH7FXp8ER6pVISfhUwhLZbIg+jl15gnb1wJZfdu5iG5jQAK9c3cNm9iwH6ddKPWyM3a8HbXDZ7MVubMwBs3NpCMgHf/sQYJXuRIkpJ+FcAz5pZA5AiGAb6VqRRddO1j3hrss9paE7z7T+8yNUPLKE6laSmKklNKkl1Kkl1VaJ1uX15kupUgtqqcL3dNonW5epU+J6qBDWpFNWpBNVVSWpb91FsP0Hdjt56N4pGLpPJ0pTOBK+WDM3pDM0tWZrSaZpagrqgLENj+LOtLNu+LHx/Y94+mluyeWVF3pu33NRaliWdyW4bcxZ+9fhyvjphVLf/LUX6uy4Tvrv/D3CAme0OpN19XTilstepX9/QYXkW+Myhw2lOtyWNppY0zekg8TS1ZNjc2JKX2PISXetyUF5uyQR5DUdbA5Irq8lrOGqqUq0NTq7heGjxqg4bucvuXcz/LH2nbAl1R+UauZqq3LkkOzzHXWqqqcmrz28gc+/9+WPLOjxGZ79/EQl0mfDN7Ajgu4QPPTGzGoJhnd9GG9r2Gz6kjpUd/KcfMaSOH5x68A7vP5vN0pLJtjYGTXmNQXN+0swra9ym4cjQlLd9ayPUSe+3ubWByrCxoXmbRmhLU7rDWBua0yxdtbE1UeY+vQQJNUlNXqNSXZiMU6nWTz/tP8kku3hv/j7ayqqSibI+8Hv2wvoOf8/Dh9SV7Rgi/VEpQzo3AJcDPyW4e+ZpwLNRBtVd0yZbu+ENgLrqFNMmW1n2n0gkgmGbXnSv9KN/8minjdyjFx/f8wH1gKh/zyL9VSmZa4u7PwY0uvsCd78SuCDiuLrl1MNHcM3phzBiSB0JgqR3zemH9OsLmNMmG3XV7e8J09+TXxx/zyLlUEoPf4uZfRZYYWY/BpYB+0QbVvedeviIWP3Hz51rnGbpQPx+zyLlUErC/zLBPXUuAC4CDgXOijIo2T5KfiJSilISfgI4CTCCCS+vAEujDEpERMqvlDH8e4APAo8BTwD7A7OiDEpERMqvlB5+rbt/J2/9bjP7S1QBiYhINEpJ+I+a2ZkET6VKEtzS+Fkz2wnA3bdEGJ+IiJRJKQm/s2/VTiEY09+3fOGIiEhUSrm1wuieCERERKJVyq0Vvgl8HdiF9o8pVM9eRKQPKWVI5wKCJ1K9E3EsIiISoVIS/nMEt1d4P+pgREQkOqUk/JeAN8zsHaCFYFgnqyEdEZG+pZSE/w2C59qu2t6dm9kMYALBbJ4L3X1+Xt1U4CtAGnje3S/a3v2LiEjpSkn4zwDvbu+QjpkdB4xx96PMbCxwM3BUWDcYmAbs7+4tZjbHzCa4e6+87bKISH9QSsLfj2BIZxnth3SO7OJ9k4DZAO6+1MyGmtlgd98INIWvQWa2GdiJtmfmiohIBEpJ+F/t5r6HAQvy1teGZRvdfauZXQ0sBxqAO939tW4eZ1u33go331y23YmI9Khzz4Wzyn9T4k4Tvpmd3MV739jOY7XO4Q+HdC4HDgA2Ety+4TB3X7Sd+xQRkRIV6+GfWaQuCzzUxb7rCXr0OcNpu/A7Flju7u8CmNlTwHigPAn/rLMiaR1FRPqyThO+u39tB/c9B7gauMnMxgH17r4prHsdGGtmde7eABxB1w2IiIjsgFLG8LvF3eeZ2QIzmwdkgKlmdg6wwd1nmdm1wGNm1gLMc/enoopFREQiTPgA7n5pQdGivLqbgJuiPL6IiLQp5YlXmFmtmY2KOBYREYlQlwnfzL5EML3yT+H6TDPTFVERkT6mlB7+VGAcwTx6gEuAb0UWkYiIRKKUhJ929yaCqZgAjRHGIyIiESkl4T9tZr8DRprZd4GnAD3EXESkjynlEYdXmtkxwGKC+99Mc/dnIo9MRETKqpRHHN7t7mcAT+eVPevuEyKNTEREyqrYvXQ+D1wKHGZma/KqUsDCqAPrlqevgxHjYPSxbWUrnoSVL8Axut2+iMRbp2P47n6Pu38EuNzd98h77Qp8u+dC3A4jxsFd5wRJHoKfd50TlPdXT1/Xdr45K54MyvurOJ6zSBmU8k3b34RPp9o1XK8Bzgb2jiyq7hp9LJx5C9zxL7DLSFi3AkZNhJfvgVfuh1Q1JKuCV6oaktWQCtdbl6s73i5ZVby+3X6q8+rC8kSiy/C7JdfInXlLcP65Ru7MW6I5Xm8Qx3MWKYNSEv4fgXnAl4BfA8cBF0QZ1A4ZfSzsdRi88VeoHQxrl8LqlyDTDOkWyLQEy5mWno2rXWOQ6qLh2M4GZtREuO0LMOzQ4FztU/DaI+B/hmwGyAY/s9mC9VxZtoRtcmXZtvWi+852UtbBereOD9z6Oaj7AGzdAB88Ojjn+oUwcA8YlHvtCTvtGvybi8RcKQk/6e7/bmbHuft0M7sR+ANwX8Sxdc+KJ2Htq3DsJfD8b+C0X7Uf08/JZoOknw6Tf+tyuJ4OG4bWsnTbcn7DkXt/636a2967Q/tpgZYmyLwflqXbb5cfb7oF0o3w9nOQSAWJjwQkksEni0SiYD0ZvArLSLRtX3SbZPvtOt0mGSTako9PiTGG66tehNWLYefh8M8V8NZz0NKw7e86kQyS/qA9YeDuwc9B4c+Be7Rf3mlXSJZ0xxGRPqeUhF9jZocBW8zsEwRPqdo/2rC6Kf+j/ehjYfTE9uv5Eomgt5yq7vk4yy133kecFzRyHZ1vf7PiSfCH2hr2028KPuk0bYbNa4LX+2valje/A++vDZbfWxbUtWzddr+JFAzcLfh0kP9JYeAebQ1FbrluqBoH6VNKSfhTgT2A7wLXE4zlXx9lUN228oX2yS43pr/yhf6bALenkesvujrn2p1h1/2K7yObhcaNsHlt2Bis6bihWOvBerpp230kq4JPDK2fGvZov5zfaNQN3bHrOHGbgRa38+0hpST8T7v7NeHyCVEGs8M6+kMYfWz/TXwQz0auHOecSMCAXYLXbl18YM1mYev6gsahg4binSXBekfXh5LVYWPQxaeGQbvDgCHbNg5xu1Adt/OFHmnkEtlstugGZjaD4GlU8wm+aQuAu28pSwSdH3cUsGLu3LmMHDkyykOJlE82Cw3/LDKk9E5YFw4vZdPb7iNVU3BtIfzZuBFevB32mwTLHoWjvgW7fyh34LbjdxRTh9ts73q59lHiPt/9Byy6A/b+aHB9ZtxXYXcLht2SVeH1oVQwrJasCpeLlVUF5bmy1vpkB/tMhdegOiiLasZd4SfXwvUSvf3220yaNAlgtLu/nl9XUg8fOLWgLAvsW3IEInGRSMBOHwhefKj4tplM2DgUGVLasDKYefT+2nCmErA0nC/xxE8jPZVeY/mjwc/nflXZOHK2qxEpoWHJX/7AfvD7z8OII+BdL/vQbCn30jmgsCx8VKGI7IhkEgbuGrw4sPi2mTS8+iDc/7/hoNNgySyY/CMYMR4Ie5ytPc+8HmhhWWHvtNP67uyzm+udxfXGPLjvAjj8K7Dw93DK9TDyI8Gnokw6GDrLZoLlbLieSXdSlg4a2ExL2/tb95MuYZ/h+8uyz4KydFP7fdbuAm/OCyYklHlYtpR76RxBcME2/4tXw4BbSnjvDGACwSeCC919flg+Argtb9N9gUvd/fbtCV4kNt74K/zpIvji74IkcPDp/fvi/Iong8btC78Nzm/MJ/r3+ebkhnFys89GTyzr+ZYyp+wG4BfAIGAa8DjQ5RUEMzsOGOPuRwHnATNzde6+0t2Pd/fjgROBN4H7tzd4kdgodqG6P4rb+UL7MfsTrgh+5t8qpgxKSfhb3P0xoNHdF7j7lZT2TdtJwGwAd18KDDWzwR1sdw5wj7tvLjFmkfg55qJte3qjj+2/UxTjdr7QI41cKRdtt5jZZ4EVZvZjYBmwTwnvG0bwLNyctWHZxoLtzgc+WcL+RET6rx6YVl5KD//LwCsEvfqtwKFAdx5ivs1cJjM7CnjV3QsbARERKbNSevgJ4CTACC6+vgIsLeF99QQ9+pzhwKqCbT6DHpcoItIjSunh3wN8EHgMeILgPjqzSnjfHOAMADMbB9S7+6aCbT4CLCo5WhER6bZSevi17v6dvPW7zazLXrm7zzOzBWY2D8gAU8P5+xvcPddg7AWs6WwfIiJSPqUk/EfN7ExgLsEngonAs2a2ExS/xYK7X1pQtKig/pDtC1dERLqrlIR/diflU9AtFkRE+oxSbq0wOrdsZilgsLv/M9KoRESk7Eq5tcKlwD8JboXwOLDOzJ5x93+PODYRESmjUmbpnOLuNwH/Atzn7p8Ejo42LBERKbdSEn7KzJIEX8D6Q1i2c3QhiYhIFEpJ+LOA1cAr7v6amX0PeC7asEREpNxKuWj7UyD/SQvXdfAFKhER6eU6Tfhm9kt3/6aZzaf1mWOtdbj7kZFHJyIiZVOsh39V+POMHohDREQi1mnCd/d3zOxgYCrB89fSwEJghru/3UPxiYhImXR60dbMTiCYlfMk8HWCp129BvwlrBMRkT6k2JDOZQRz8JfnlS0Ib5x2G8GzakVEpI8oNi2zuiDZA+DuywjufikiIn1IsYRfLKlvLXcgIiISrWJDOkeY2d86KE8AB0QUj4iIRKRYwte96kVE+pFi0zLf6MlAREQkWqXcS0dERPqBUp541W1mNoNg+mYWuNDd5+fV7Q3cAdQAL7j7N6KMRUQk7iLr4ZvZccAYdz8KOA+YWbDJdGB6eE+etJntE1UsIiIS7ZDOJGA2gLsvBYaa2WCA8P76E4H7w/qp7v5mhLGIiMRelAl/GLA2b31tWAawO7AJmGFmT5vZNRHGISIi9OxF20TB8gjgeuA44HAz+3QPxiIiEjtRJvx62nr0AMOBVeHyu8Ab7r7M3dPAXOCgCGMREYm9KBP+HMJ76ZvZOKA+96Qsd28BlpvZmHDb8YBHGIuISOxFNi3T3eeZ2QIzm0dwX56pZnYOsMHdZwEXAbeEF3AXAw9EFYuIiEQ8D9/dLy0oWpRX9w/gmCiPLyIibfRNWxGRmFDCFxGJCSV8EZGYUMIXEYkJJXwRkZhQwhcRiQklfBGRmFDCFxGJCSV8EZGYUMIXEYkJJXwRkZhQwhcRiQklfBGRmFDCFxGJCSV8EZGYUMIXEYkJJXwRkZhQwhcRiYlIH3FoZjOACUAWuNDd5+fVvQ68BaTDoinuvjLKeERE4iyyhG9mxwFj3P0oMxsL3AwcVbDZSe6+OaoYRESkTZRDOpOA2QDuvhQYamaDIzyeiIgUEeWQzjBgQd762rBsY17Zr8xsFPA0cJm7ZyOMR0Qk1nryom2iYP37wL8BxwMHA5/vwVhERGInyh5+PUGPPmc4sCq34u635pbN7CHgEODuCOMREYm1KHv4c4AzAMxsHFDv7pvC9V3M7BEzqwm3PQ54OcJYRERiL7IevrvPM7MFZjYPyABTzewcYIO7zwp79c+aWQOwEPXuRUQiFek8fHe/tKBoUV7d9cD1UR5fRETa6Ju2IiIxoYQvIhITSvgiIjGhhC8iEhNK+CIiMaGELyISE0r4IiIxoYQvIhITSvgiIjGhhC8iEhNK+CIiMaGELyISE0r4IiIxoYQvIhITSvgiIjGhhC8iEhNK+CIiMaGELyISE0r4IiIxEekzbc1sBjAByAIXuvv8Dra5BjjK3Y+PMhYRkbiLrIdvZscBY9z9KOA8YGYH2xwIHBtVDCIi0ibKHv4kYDaAuy81s6FmNtjdN+ZtMx24Ariqg/enAFavXh1hiCIi/UtezkwV1kWZ8IcBC/LW14ZlGwHM7BzgCeD1Tt6/F8CUKVMiC1BEpB/bC1iWXxDpGH6BRG7BzD4AfA04ERjRyfbzgYnAKiAdeXQiIv1DiiDZb3PNNMqEX0/Qo88ZTpC8AU4AdgeeAmqB/cxshrt/O7exuzcCT0cYn4hIf7Wso8JENpuN5Ghm9jHganf/hJmNA2a6+zEdbDcKuEWzdEREohVZD9/d55nZAjObB2SAqeG4/QZ3n1Wu4xSb+mlmJwI/JhgSesjdf1Cu41ZSF+f8ceAagnN24Hx3z1Qk0DKK4xTfLn7PewN3ADXAC+7+jcpEWV5dnPNU4CsEf9vPu/tFlYmyvMzsYOA+YIa731hQV9YcFukXr9z9Unf/mLsf4+6L3P2WwmTv7q939z9oCVM/ZwKfB44GPhlOA+3TSjjnXwNnuPvRwM7Ap3o4xLKL4xTfEs55OjDd3Y8E0ma2T0/HWG7FztnMBgPTgInhSMGBZjahMpGWj5kNBG4A5naySVlzWF//pm27qZ/A0PAPAzPbF1jn7m+FPdyHwu37uk7POTTe3d8Ol9cCu/ZwfFHo6pyhbYpvf1HsbztJMKHh/rB+qru/WalAy6jY77kpfA0ysypgJ2BdRaIsr0bgZIJrnu1EkcP6esIfRpDUcnJTPzuqW0M41bOPK6Iy5ekAAAQ2SURBVHbO5L7nYGZ7AZ8k+CPp64qecwlTfPuiYue8O7AJmGFmT4dDWf1Bp+fs7luBq4HlwBvAc+7+Wo9HWGbu3uLuDZ1Ulz2H9fWEXyjRzbq+bJvzMrM9gAeAb7n7ez0fUuQ6muI7vXLh9IhEwfII4HrgOOBwM/t0RaKKVv7veTBwOXAAMBr4qJkdVqnAKmSHc1hfT/jFpn4W1o2gg49NfVCxc879x3gYuNLd5/RwbFEpdYrvLGBceOGvryt2zu8Cb7j7MndPE4z/HtTD8UWh2DmPBZa7+7vu3kTw+x7fw/H1tLLnsL6e8OcAZwCEUz/r3X0TBBeDgcFmNioc8/tMuH1f1+k5h6YTXO3/cyWCi0ix3/Pd7n6gu08ATiOYsfLtznfVZxQ75xZguZmNCbcdTzAjq68r9rf9OjDWzOrC9SOAv/d4hD0oihwW2Tz8nmJmPyGYnZEBpgKHE079NLNjgZ+Gm97j7j+rUJhl1dk5A48A/wSeydv8dnf/dY8HWWbFfs9524yiH32no4u/7f2BWwg6bYuBb/aT6bfFzvlfCYbvWoB57n5J5SItDzMbT9BJGwU0AysJLsaviCKH9fmELyIipenrQzoiIlIiJXwRkZhQwhcRiQklfBGRmFDCFxGJCSV8kRKY2TlmdlqR+uPN7O4yHevx8A6KImXVk0+8Eumz3P2WSscgsqOU8KVfCm+odhLB1/O/BJwKfJngCz2z3X26mV0FjAT2Ibgp1TR3/7OZXUzwjc8kwT3Irw63fRd4GfgOMAi42N3zn9ucO/bpwMUEXxB63t0vNrMXgFPd/U0z+yBwL3Akwe2s9wWqge+7+6NR/HuIgIZ0pH/bh+BbmzUECfyYcP3zefePH+HunyRoDPLvOnkMwYM4zungVsyHAJM7SfaDgCuBE9z9OGBvMzua4D4/p4SbfQ64JzzmKnf/OEGDdN0Onq9IUUr40p/Nd/csQU96DPBY+NqZ4KvsED54wt0XE9ycCmALwe2WHwN2Az5QsN9F4TOXO3IQQUPziJk9Hh4316PPT/h3Ax8DTg23uxuoM7Oa7p2qSNc0pCP9WVPezwfd/V/zK83sBAo6PeFwy78Bh7v7ZjN7uch+OzvmAnefXFhhZsPDRxMOcffXzKwJ+JG731GwXVfnJdIt6uFLHCwAPm5mO5lZwsyuz7vr4jEAZnYowYM1dgPWhMl+HEHvfHt63U5wV8c9wv1ebWa5Tw4PAj8ieH4pwHMEvX3MbA8z+3H3T1Gka0r40u+Fj/+7DngSeBZYnfeUoY1mdj9wG3Ap8CKw2cz+CnwRuAn4xXYcawtwEfBQuI9dabuH+b0E4/a56Zt/DI81j+CBNU91+yRFSqC7ZUps5WbeuPuNlY5FpCeohy8iEhPq4YuIxIR6+CIiMaGELyISE0r4IiIxoYQvIhITSvgiIjGhhC8iEhP/H0mUS2EMmXGJAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[0.8730737561527749, 0.8675821918772759, 0.8731762794119393, 1.0246725375957972, 1.0328287617597163, 1.0272046914940403]\n",
            "[0.7429408073624765, 0.7387575820957959, 0.7401171303074671, 0.719619326500732, 0.7200376490274001, 0.7172139719723907]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75f9qpEx6ey_"
      },
      "source": [
        "These are the results when running the DIR on the whole dataset:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjw5MpFl6iB5"
      },
      "source": [
        "<img src= 'https://github.com/BiancaZimmer/Stat-ML-Fairness/raw/22c2b912580e460eecb23b95ce4ffe1a51c59634/DIR_wholeDataset.png' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOqrNQId66gj"
      },
      "source": [
        "We can see that the DI only slightly worsens from repair level 0 to 0.2 afterwards the DI slightly improves with each repair level step. The improvement is only 0.0364. However our DI is greater than 0.8 and thus deemed acceptable. Improvement is always possible, though. The accuracy drops as expected but also only very slightly from 74.11% to 74.08%.   \n",
        "The DIR does not seem to work very well with our dataset as it does not improve the DI profoundly. However when we look at a smaller portion of the dataset the DIR seems to work quite well or even overshoot (DI > 1). It might be that DIR is a method for smaller datasets and not well suited for bigger ones. This a topic that should be explored in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vB6V9z16U4W"
      },
      "source": [
        "## 4.2 In-Processing\n",
        "In-processing happens during training. The model itself is modified by changing the objective function or imposing certain constraints. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibdJ6azIpT6a"
      },
      "source": [
        "###5. Adversarial Debiasing\n",
        "\n",
        "Adversarial Debiasing (Zhang et al. 2018) is a supervised deep learning approach, where two networks are built: \n",
        "1. The predictor model makes predictions based on the regular data set.\n",
        "2. The adversary model takes the output of the predictor model as input and tries to predict the protected attribute. \n",
        "\n",
        "Both networks then compete against each other while the predictor's goal is to prevent the adversary from determining the protected attribute.   \n",
        "\n",
        "Since Adverserial Debiasing (AD) is a method based on deep neural networks (DNN) we can not use our random forest classifier as a basis. We will use the implemented DNNs which AIF360 uses for the AD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYqkUWIYpWqe"
      },
      "source": [
        "from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_eager_execution()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y9ijhVA4o_5"
      },
      "source": [
        "As a reminder we compute the Statistical Parity differences for the test and training datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "h7KAd0lephCx",
        "outputId": "81bf602c-0c70-4f16-d7b6-66817744d315"
      },
      "source": [
        "# Metric for the original dataset\n",
        "metric_orig_train = BinaryLabelDatasetMetric(data_race_train, \n",
        "                                             unprivileged_groups=unprivileged_groups,\n",
        "                                             privileged_groups=privileged_groups)\n",
        "display(Markdown(\"#### Original training and test dataset\"))\n",
        "print(\"Train set: Difference in statistical parity outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())\n",
        "metric_orig_test = BinaryLabelDatasetMetric(data_race_test, \n",
        "                                             unprivileged_groups=unprivileged_groups,\n",
        "                                             privileged_groups=privileged_groups)\n",
        "print(\"Test set: Difference in statistical parity outcomes between unprivileged and privileged groups = %f\" % metric_orig_test.mean_difference())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Original training and test dataset",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Train set: Difference in statistical parity outcomes between unprivileged and privileged groups = -0.077098\n",
            "Test set: Difference in statistical parity outcomes between unprivileged and privileged groups = -0.075544\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWYaQihytKFs"
      },
      "source": [
        "As stated above we now have to divert from our random forest classifier and first train a plain DNN model with tensorflow. For this purpose we first specify all our parameters for the DNN:   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_2wbTr7qV4w"
      },
      "source": [
        "#tf.reset_default_graph() # use if you want to rerun the model\n",
        "sess = tf.Session()\n",
        "plain_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
        "                          unprivileged_groups = unprivileged_groups,\n",
        "                          scope_name='plain_classifier',\n",
        "                          debias=False,\n",
        "                          sess=sess,\n",
        "                          num_epochs = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m36MHDfJ-lt4"
      },
      "source": [
        "Since AD needs a few minutes (about 5) to run on the whole dataset you can set `whole_dataset = False` to generate quicker results on a smaller subset of the data. However be aware that AD is a method using DNN and thus generally performs better the more data it has as an input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h06Gk3_h9eVV"
      },
      "source": [
        "whole_dataset = True\n",
        "\n",
        "if whole_dataset:\n",
        "  data_AD_train = data_race_train.copy()\n",
        "  data_AD_test = data_race_test.copy()\n",
        "else:\n",
        "  np.random.seed(3456)\n",
        "  d_train_min, _ = data_race_train.split([0.2], shuffle=True)\n",
        "  d_test_min, _ = data_race_test.split([0.2], shuffle=True)\n",
        "  data_AD_train = d_train_min\n",
        "  data_AD_test = d_test_min"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxLxI0Fg5RIQ"
      },
      "source": [
        "Now let's train our plain DNN on the training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-S9P265uufro",
        "outputId": "2dbf229b-c788-4a24-ae72-39e82a01a72e"
      },
      "source": [
        "plain_model.fit(data_AD_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 0; iter: 0; batch classifier loss: 129.387970\n",
            "epoch 0; iter: 200; batch classifier loss: 27.149780\n",
            "epoch 0; iter: 400; batch classifier loss: 8.194244\n",
            "epoch 0; iter: 600; batch classifier loss: 1.868647\n",
            "epoch 0; iter: 800; batch classifier loss: 1.334041\n",
            "epoch 0; iter: 1000; batch classifier loss: 0.782996\n",
            "epoch 0; iter: 1200; batch classifier loss: 0.706884\n",
            "epoch 0; iter: 1400; batch classifier loss: 0.680930\n",
            "epoch 0; iter: 1600; batch classifier loss: 0.699754\n",
            "epoch 0; iter: 1800; batch classifier loss: 0.645440\n",
            "epoch 0; iter: 2000; batch classifier loss: 0.787780\n",
            "epoch 0; iter: 2200; batch classifier loss: 0.561749\n",
            "epoch 0; iter: 2400; batch classifier loss: 0.556623\n",
            "epoch 0; iter: 2600; batch classifier loss: 0.661307\n",
            "epoch 0; iter: 2800; batch classifier loss: 0.687406\n",
            "epoch 0; iter: 3000; batch classifier loss: 0.583508\n",
            "epoch 0; iter: 3200; batch classifier loss: 0.551651\n",
            "epoch 0; iter: 3400; batch classifier loss: 0.653563\n",
            "epoch 1; iter: 0; batch classifier loss: 0.667216\n",
            "epoch 1; iter: 200; batch classifier loss: 0.597868\n",
            "epoch 1; iter: 400; batch classifier loss: 0.561292\n",
            "epoch 1; iter: 600; batch classifier loss: 0.645260\n",
            "epoch 1; iter: 800; batch classifier loss: 0.543891\n",
            "epoch 1; iter: 1000; batch classifier loss: 0.612620\n",
            "epoch 1; iter: 1200; batch classifier loss: 0.626664\n",
            "epoch 1; iter: 1400; batch classifier loss: 0.577397\n",
            "epoch 1; iter: 1600; batch classifier loss: 0.598666\n",
            "epoch 1; iter: 1800; batch classifier loss: 0.599822\n",
            "epoch 1; iter: 2000; batch classifier loss: 0.646013\n",
            "epoch 1; iter: 2200; batch classifier loss: 0.615092\n",
            "epoch 1; iter: 2400; batch classifier loss: 0.603796\n",
            "epoch 1; iter: 2600; batch classifier loss: 0.606085\n",
            "epoch 1; iter: 2800; batch classifier loss: 0.597580\n",
            "epoch 1; iter: 3000; batch classifier loss: 0.547176\n",
            "epoch 1; iter: 3200; batch classifier loss: 0.516343\n",
            "epoch 1; iter: 3400; batch classifier loss: 0.551475\n",
            "epoch 2; iter: 0; batch classifier loss: 0.584850\n",
            "epoch 2; iter: 200; batch classifier loss: 0.520152\n",
            "epoch 2; iter: 400; batch classifier loss: 0.630024\n",
            "epoch 2; iter: 600; batch classifier loss: 0.624321\n",
            "epoch 2; iter: 800; batch classifier loss: 0.636412\n",
            "epoch 2; iter: 1000; batch classifier loss: 0.631206\n",
            "epoch 2; iter: 1200; batch classifier loss: 0.617326\n",
            "epoch 2; iter: 1400; batch classifier loss: 0.595777\n",
            "epoch 2; iter: 1600; batch classifier loss: 0.591266\n",
            "epoch 2; iter: 1800; batch classifier loss: 0.599781\n",
            "epoch 2; iter: 2000; batch classifier loss: 0.586090\n",
            "epoch 2; iter: 2200; batch classifier loss: 0.538662\n",
            "epoch 2; iter: 2400; batch classifier loss: 0.596249\n",
            "epoch 2; iter: 2600; batch classifier loss: 0.520830\n",
            "epoch 2; iter: 2800; batch classifier loss: 0.523211\n",
            "epoch 2; iter: 3000; batch classifier loss: 0.600761\n",
            "epoch 2; iter: 3200; batch classifier loss: 0.582602\n",
            "epoch 2; iter: 3400; batch classifier loss: 0.575505\n",
            "epoch 3; iter: 0; batch classifier loss: 0.583232\n",
            "epoch 3; iter: 200; batch classifier loss: 0.530527\n",
            "epoch 3; iter: 400; batch classifier loss: 0.587242\n",
            "epoch 3; iter: 600; batch classifier loss: 0.564890\n",
            "epoch 3; iter: 800; batch classifier loss: 0.626658\n",
            "epoch 3; iter: 1000; batch classifier loss: 0.534983\n",
            "epoch 3; iter: 1200; batch classifier loss: 0.605278\n",
            "epoch 3; iter: 1400; batch classifier loss: 0.589962\n",
            "epoch 3; iter: 1600; batch classifier loss: 0.561944\n",
            "epoch 3; iter: 1800; batch classifier loss: 0.682152\n",
            "epoch 3; iter: 2000; batch classifier loss: 0.547923\n",
            "epoch 3; iter: 2200; batch classifier loss: 0.630843\n",
            "epoch 3; iter: 2400; batch classifier loss: 0.632372\n",
            "epoch 3; iter: 2600; batch classifier loss: 0.617033\n",
            "epoch 3; iter: 2800; batch classifier loss: 0.537693\n",
            "epoch 3; iter: 3000; batch classifier loss: 0.624445\n",
            "epoch 3; iter: 3200; batch classifier loss: 0.591880\n",
            "epoch 3; iter: 3400; batch classifier loss: 0.604673\n",
            "epoch 4; iter: 0; batch classifier loss: 0.599716\n",
            "epoch 4; iter: 200; batch classifier loss: 0.610551\n",
            "epoch 4; iter: 400; batch classifier loss: 0.497880\n",
            "epoch 4; iter: 600; batch classifier loss: 0.587766\n",
            "epoch 4; iter: 800; batch classifier loss: 0.551271\n",
            "epoch 4; iter: 1000; batch classifier loss: 0.590657\n",
            "epoch 4; iter: 1200; batch classifier loss: 0.572428\n",
            "epoch 4; iter: 1400; batch classifier loss: 0.544338\n",
            "epoch 4; iter: 1600; batch classifier loss: 0.539823\n",
            "epoch 4; iter: 1800; batch classifier loss: 0.538263\n",
            "epoch 4; iter: 2000; batch classifier loss: 0.537439\n",
            "epoch 4; iter: 2200; batch classifier loss: 0.541566\n",
            "epoch 4; iter: 2400; batch classifier loss: 0.530340\n",
            "epoch 4; iter: 2600; batch classifier loss: 0.587119\n",
            "epoch 4; iter: 2800; batch classifier loss: 0.675787\n",
            "epoch 4; iter: 3000; batch classifier loss: 0.577862\n",
            "epoch 4; iter: 3200; batch classifier loss: 0.514870\n",
            "epoch 4; iter: 3400; batch classifier loss: 0.575673\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<aif360.algorithms.inprocessing.adversarial_debiasing.AdversarialDebiasing at 0x7f40806441d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNLYNpRbtsLh"
      },
      "source": [
        "The classifier loss looks good. It already drops profoundly during the first epoch. We probably would not have needed to train for 25 epochs. When you are training the DNN on a smaller subset of the data it needs a few more epochs to reach the same loss value.   \n",
        "\n",
        "We now apply our plain model to the train and test data set and compare the metrics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Mpo6phlvCD7"
      },
      "source": [
        "# Apply the plain model to test data\n",
        "dataset_nodebiasing_train = plain_model.predict(data_AD_train)\n",
        "dataset_nodebiasing_test = plain_model.predict(data_AD_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUrhhfQSuB2F"
      },
      "source": [
        "def calc_ADmetrics(dataset_train_pred, dataset_test_pred):\n",
        "  display(Markdown(\"#### Dataset metrics\"))\n",
        "  metric_dataset_train_pred = BinaryLabelDatasetMetric(dataset_train_pred, \n",
        "                                              unprivileged_groups=unprivileged_groups,\n",
        "                                              privileged_groups=privileged_groups)\n",
        "\n",
        "  print(\"Train set: Difference in statistical parity outcomes between unprivileged and privileged groups = %f\" % metric_dataset_train_pred.mean_difference())\n",
        "\n",
        "  metric_dataset_test_pred = BinaryLabelDatasetMetric(dataset_test_pred, \n",
        "                                              unprivileged_groups=unprivileged_groups,\n",
        "                                              privileged_groups=privileged_groups)\n",
        "\n",
        "  print(\"Test set: Difference in statistical parity outcomes between unprivileged and privileged groups = %f\" % metric_dataset_test_pred.mean_difference())\n",
        "\n",
        "  display(Markdown(\"#### Classification metrics\"))\n",
        "  classified_metric_test = ClassificationMetric(data_AD_test, #careful, hardcoded\n",
        "                                                  dataset_test_pred,\n",
        "                                                  unprivileged_groups=unprivileged_groups,\n",
        "                                                  privileged_groups=privileged_groups)\n",
        "  print(\"Test set: Classification accuracy = %f\" % classified_metric_test.accuracy())\n",
        "  TPR = classified_metric_test.true_positive_rate()\n",
        "  TNR = classified_metric_test.true_negative_rate()\n",
        "  bal_acc_nodebiasing_test = 0.5*(TPR+TNR)\n",
        "  print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_nodebiasing_test)\n",
        "  print(\"Test set: Disparate impact = %f\" % classified_metric_test.disparate_impact())\n",
        "  print(\"Test set: Equal opportunity difference = %f\" % classified_metric_test.equal_opportunity_difference())\n",
        "  print(\"Test set: Average odds difference = %f\" % classified_metric_test.average_odds_difference())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "XdLPcNpbu_9_",
        "outputId": "e960cd78-d1c4-4c86-b560-6cda8b6ab80c"
      },
      "source": [
        "display(Markdown(\"#### Plain model - without debiasing\"))\n",
        "calc_ADmetrics(dataset_nodebiasing_train, dataset_nodebiasing_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Plain model - without debiasing",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Dataset metrics",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Train set: Difference in statistical parity outcomes between unprivileged and privileged groups = 0.000000\n",
            "Test set: Difference in statistical parity outcomes between unprivileged and privileged groups = 0.000000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Classification metrics",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Test set: Classification accuracy = 0.702480\n",
            "Test set: Balanced classification accuracy = 0.500000\n",
            "Test set: Disparate impact = 1.000000\n",
            "Test set: Equal opportunity difference = 0.000000\n",
            "Test set: Average odds difference = 0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc3nhduOvfZc"
      },
      "source": [
        "When we look at the results we notice that Statistical Parity has already dropped to 0 although we didn't even use AD but simply trained a DNN. When looking at the classification accuracy we can see that it is at 70.25% which is equal to the proportion of solved crimes. Our DNN didn't learn anything and is simply guessing. Since we do not focus on finding a good model in this jupyter notebook but rather on using the bias mitigation methods we did not investigate further how to improve the model.   \n",
        "Another reason why we didn't try to improve the DNN model is that the AD function implemented in AIF360 seems to be very inflexible. One would have to rewrite parts of the base code of their function to actually improve the DNN. This is out of the scope of this project.   \n",
        "If you still want to use AD, we would advise you to improve the DNN by getting more training data, tweaking the learning rate and batch size as well as the underlying layers by e.g. adding a dropout layer.   \n",
        "Another big drawback of the implementation of the AIF360 function is not being able to optimize for validation loss since there is no possibility to put in a validation dataset as a parameter into the function.  \n",
        "\n",
        "To be fair: A model which is only guessing the response variable is the fairest model one can get. You can also see this looking at the bias metrics: None of them indicate any bias. However a model which assigns the response by chance is of course useless in practice. \n",
        "\n",
        "Even though we a have a model which guesses whether a crime was solved or not we still want to show how an implementation for AD would look like.\n",
        "\n",
        "We now re-train our DNN model with the debiasing option turned on (`debias = True`) and again compute the metrics for train and test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnHtHFk1xS8x"
      },
      "source": [
        "# important to close and reopen tesorflow session for a new tf model\n",
        "sess.close()\n",
        "tf.reset_default_graph()\n",
        "sess = tf.Session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qS_8t0DxYTr"
      },
      "source": [
        "# Learn parameters with debias set to True\n",
        "debiased_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
        "                          unprivileged_groups = unprivileged_groups,\n",
        "                          scope_name='debiased_classifier',\n",
        "                          debias=True,\n",
        "                          sess=sess,\n",
        "                          num_epochs = 5)\n",
        "# parameter adversary_loss_weight = 0.1 not used because no effect can be seen in our context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUjqNSdpxf8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0698960-d2ae-4935-c281-3e9c3a63a4a1"
      },
      "source": [
        "debiased_model.fit(data_AD_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0; iter: 0; batch classifier loss: 180.321640; batch adversarial loss: 0.811245\n",
            "epoch 0; iter: 200; batch classifier loss: 33.545601; batch adversarial loss: 0.842707\n",
            "epoch 0; iter: 400; batch classifier loss: 3.947190; batch adversarial loss: 0.742144\n",
            "epoch 0; iter: 600; batch classifier loss: 2.890715; batch adversarial loss: 0.695410\n",
            "epoch 0; iter: 800; batch classifier loss: 1.312801; batch adversarial loss: 0.689426\n",
            "epoch 0; iter: 1000; batch classifier loss: 1.283877; batch adversarial loss: 0.693547\n",
            "epoch 0; iter: 1200; batch classifier loss: 0.760577; batch adversarial loss: 0.697991\n",
            "epoch 0; iter: 1400; batch classifier loss: 0.871973; batch adversarial loss: 0.696279\n",
            "epoch 0; iter: 1600; batch classifier loss: 0.648640; batch adversarial loss: 0.675771\n",
            "epoch 0; iter: 1800; batch classifier loss: 0.640385; batch adversarial loss: 0.671011\n",
            "epoch 0; iter: 2000; batch classifier loss: 0.662157; batch adversarial loss: 0.702880\n",
            "epoch 0; iter: 2200; batch classifier loss: 0.648809; batch adversarial loss: 0.680688\n",
            "epoch 0; iter: 2400; batch classifier loss: 0.628456; batch adversarial loss: 0.687946\n",
            "epoch 0; iter: 2600; batch classifier loss: 0.633541; batch adversarial loss: 0.679518\n",
            "epoch 0; iter: 2800; batch classifier loss: 0.642290; batch adversarial loss: 0.699160\n",
            "epoch 0; iter: 3000; batch classifier loss: 0.694920; batch adversarial loss: 0.697867\n",
            "epoch 0; iter: 3200; batch classifier loss: 0.546770; batch adversarial loss: 0.698505\n",
            "epoch 0; iter: 3400; batch classifier loss: 0.645918; batch adversarial loss: 0.683038\n",
            "epoch 1; iter: 0; batch classifier loss: 0.616575; batch adversarial loss: 0.708143\n",
            "epoch 1; iter: 200; batch classifier loss: 0.696229; batch adversarial loss: 0.692771\n",
            "epoch 1; iter: 400; batch classifier loss: 0.609647; batch adversarial loss: 0.681786\n",
            "epoch 1; iter: 600; batch classifier loss: 0.661518; batch adversarial loss: 0.682783\n",
            "epoch 1; iter: 800; batch classifier loss: 0.717794; batch adversarial loss: 0.690672\n",
            "epoch 1; iter: 1000; batch classifier loss: 0.592081; batch adversarial loss: 0.686865\n",
            "epoch 1; iter: 1200; batch classifier loss: 0.605658; batch adversarial loss: 0.692028\n",
            "epoch 1; iter: 1400; batch classifier loss: 0.584978; batch adversarial loss: 0.684264\n",
            "epoch 1; iter: 1600; batch classifier loss: 0.570053; batch adversarial loss: 0.687444\n",
            "epoch 1; iter: 1800; batch classifier loss: 0.587991; batch adversarial loss: 0.671118\n",
            "epoch 1; iter: 2000; batch classifier loss: 0.666827; batch adversarial loss: 0.682158\n",
            "epoch 1; iter: 2200; batch classifier loss: 0.653151; batch adversarial loss: 0.694730\n",
            "epoch 1; iter: 2400; batch classifier loss: 0.639215; batch adversarial loss: 0.671061\n",
            "epoch 1; iter: 2600; batch classifier loss: 0.595920; batch adversarial loss: 0.682276\n",
            "epoch 1; iter: 2800; batch classifier loss: 0.582018; batch adversarial loss: 0.688807\n",
            "epoch 1; iter: 3000; batch classifier loss: 0.594101; batch adversarial loss: 0.670636\n",
            "epoch 1; iter: 3200; batch classifier loss: 0.565065; batch adversarial loss: 0.696894\n",
            "epoch 1; iter: 3400; batch classifier loss: 0.628242; batch adversarial loss: 0.693640\n",
            "epoch 2; iter: 0; batch classifier loss: 0.592303; batch adversarial loss: 0.677946\n",
            "epoch 2; iter: 200; batch classifier loss: 0.554075; batch adversarial loss: 0.698123\n",
            "epoch 2; iter: 400; batch classifier loss: 0.531261; batch adversarial loss: 0.680632\n",
            "epoch 2; iter: 600; batch classifier loss: 0.605864; batch adversarial loss: 0.688745\n",
            "epoch 2; iter: 800; batch classifier loss: 0.716892; batch adversarial loss: 0.669927\n",
            "epoch 2; iter: 1000; batch classifier loss: 0.646550; batch adversarial loss: 0.684333\n",
            "epoch 2; iter: 1200; batch classifier loss: 0.654996; batch adversarial loss: 0.683088\n",
            "epoch 2; iter: 1400; batch classifier loss: 0.580351; batch adversarial loss: 0.690438\n",
            "epoch 2; iter: 1600; batch classifier loss: 0.631923; batch adversarial loss: 0.680277\n",
            "epoch 2; iter: 1800; batch classifier loss: 0.568205; batch adversarial loss: 0.691770\n",
            "epoch 2; iter: 2000; batch classifier loss: 0.630609; batch adversarial loss: 0.687713\n",
            "epoch 2; iter: 2200; batch classifier loss: 0.672718; batch adversarial loss: 0.686058\n",
            "epoch 2; iter: 2400; batch classifier loss: 0.627019; batch adversarial loss: 0.701823\n",
            "epoch 2; iter: 2600; batch classifier loss: 0.597106; batch adversarial loss: 0.679698\n",
            "epoch 2; iter: 2800; batch classifier loss: 0.604316; batch adversarial loss: 0.694106\n",
            "epoch 2; iter: 3000; batch classifier loss: 0.592349; batch adversarial loss: 0.691915\n",
            "epoch 2; iter: 3200; batch classifier loss: 0.625260; batch adversarial loss: 0.676106\n",
            "epoch 2; iter: 3400; batch classifier loss: 0.570459; batch adversarial loss: 0.676393\n",
            "epoch 3; iter: 0; batch classifier loss: 0.546245; batch adversarial loss: 0.682847\n",
            "epoch 3; iter: 200; batch classifier loss: 0.574684; batch adversarial loss: 0.689003\n",
            "epoch 3; iter: 400; batch classifier loss: 0.620957; batch adversarial loss: 0.682132\n",
            "epoch 3; iter: 600; batch classifier loss: 0.581255; batch adversarial loss: 0.697268\n",
            "epoch 3; iter: 800; batch classifier loss: 0.649002; batch adversarial loss: 0.684693\n",
            "epoch 3; iter: 1000; batch classifier loss: 0.601687; batch adversarial loss: 0.693921\n",
            "epoch 3; iter: 1200; batch classifier loss: 0.585030; batch adversarial loss: 0.694227\n",
            "epoch 3; iter: 1400; batch classifier loss: 0.613605; batch adversarial loss: 0.678769\n",
            "epoch 3; iter: 1600; batch classifier loss: 0.673873; batch adversarial loss: 0.694326\n",
            "epoch 3; iter: 1800; batch classifier loss: 0.562635; batch adversarial loss: 0.686817\n",
            "epoch 3; iter: 2000; batch classifier loss: 0.518919; batch adversarial loss: 0.689293\n",
            "epoch 3; iter: 2200; batch classifier loss: 0.652150; batch adversarial loss: 0.705333\n",
            "epoch 3; iter: 2400; batch classifier loss: 0.531422; batch adversarial loss: 0.684566\n",
            "epoch 3; iter: 2600; batch classifier loss: 0.604906; batch adversarial loss: 0.682166\n",
            "epoch 3; iter: 2800; batch classifier loss: 0.615609; batch adversarial loss: 0.675567\n",
            "epoch 3; iter: 3000; batch classifier loss: 0.633529; batch adversarial loss: 0.685934\n",
            "epoch 3; iter: 3200; batch classifier loss: 0.574086; batch adversarial loss: 0.690900\n",
            "epoch 3; iter: 3400; batch classifier loss: 0.621744; batch adversarial loss: 0.703288\n",
            "epoch 4; iter: 0; batch classifier loss: 0.583131; batch adversarial loss: 0.685897\n",
            "epoch 4; iter: 200; batch classifier loss: 0.617484; batch adversarial loss: 0.673550\n",
            "epoch 4; iter: 400; batch classifier loss: 0.602779; batch adversarial loss: 0.691898\n",
            "epoch 4; iter: 600; batch classifier loss: 0.608778; batch adversarial loss: 0.695764\n",
            "epoch 4; iter: 800; batch classifier loss: 0.617799; batch adversarial loss: 0.688479\n",
            "epoch 4; iter: 1000; batch classifier loss: 0.615532; batch adversarial loss: 0.686278\n",
            "epoch 4; iter: 1200; batch classifier loss: 0.604447; batch adversarial loss: 0.684528\n",
            "epoch 4; iter: 1400; batch classifier loss: 0.597239; batch adversarial loss: 0.694611\n",
            "epoch 4; iter: 1600; batch classifier loss: 0.608164; batch adversarial loss: 0.690385\n",
            "epoch 4; iter: 1800; batch classifier loss: 0.543012; batch adversarial loss: 0.687165\n",
            "epoch 4; iter: 2000; batch classifier loss: 0.639927; batch adversarial loss: 0.695338\n",
            "epoch 4; iter: 2200; batch classifier loss: 0.617189; batch adversarial loss: 0.708688\n",
            "epoch 4; iter: 2400; batch classifier loss: 0.539632; batch adversarial loss: 0.692999\n",
            "epoch 4; iter: 2600; batch classifier loss: 0.572972; batch adversarial loss: 0.688887\n",
            "epoch 4; iter: 2800; batch classifier loss: 0.606210; batch adversarial loss: 0.705958\n",
            "epoch 4; iter: 3000; batch classifier loss: 0.614107; batch adversarial loss: 0.694951\n",
            "epoch 4; iter: 3200; batch classifier loss: 0.530886; batch adversarial loss: 0.679313\n",
            "epoch 4; iter: 3400; batch classifier loss: 0.620774; batch adversarial loss: 0.697791\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<aif360.algorithms.inprocessing.adversarial_debiasing.AdversarialDebiasing at 0x7f407fff99d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onDEvrSixTs6"
      },
      "source": [
        "Looking at the loss metrics which are given during the model training we can now see the loss of the classifier (our model which we will obtain at the end) and the loss of the adversarial model.    \n",
        "Both losses do not change much after the first epoch similar to the plain DNN model. However we can see that the classifier loss starts at a very high value (similiar to the plain model) whereas the adversarial loss starts at about 0.7. Remember: The adversarial model tries to predict the protected attribute with the prediction of the classifer model. We thus actually would want a pretty high loss for the adversarial model and a low loss for the classifier model. Since both are constructed as DNNs and to minimize loss those two models compete against each other.    \n",
        "We would expect the adversarial loss to increase at least a little. However since our classifier doesn't learn anything the adversarial loss stays pretty much at the same level, it even decreases a little.   \n",
        "We would have loved to show a graphic with both the losses however again the AdverserialDebiasing.fit method of AIF360 does not allow for a `callbacks` parameter (which is a standard in tensorflow) and thus we can not get to the losses without altering the base code. This is a drawback and should be implemented to AIF360 for the future.   \n",
        "\n",
        "   \n",
        "We not again apply the metrics for the debiased model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gIkvgwHxyFK"
      },
      "source": [
        "# Apply the debiased model to test data\n",
        "dataset_debiasing_train = debiased_model.predict(data_AD_train)\n",
        "dataset_debiasing_test = debiased_model.predict(data_AD_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPJMmWmhwPTs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "5fe3b476-1c5e-4514-d1a7-4dc5dee0a5c5"
      },
      "source": [
        "display(Markdown(\"#### Plain model - without debiasing\"))\n",
        "calc_ADmetrics(dataset_nodebiasing_train, dataset_nodebiasing_test)\n",
        "\n",
        "display(Markdown(\"#### Model - with debiasing\"))\n",
        "calc_ADmetrics(dataset_debiasing_train, dataset_debiasing_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Plain model - without debiasing",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Dataset metrics",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Train set: Difference in statistical parity outcomes between unprivileged and privileged groups = 0.000000\n",
            "Test set: Difference in statistical parity outcomes between unprivileged and privileged groups = 0.000000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Classification metrics",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Test set: Classification accuracy = 0.702480\n",
            "Test set: Balanced classification accuracy = 0.500000\n",
            "Test set: Disparate impact = 1.000000\n",
            "Test set: Equal opportunity difference = 0.000000\n",
            "Test set: Average odds difference = 0.000000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Model - with debiasing",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Dataset metrics",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Train set: Difference in statistical parity outcomes between unprivileged and privileged groups = 0.000000\n",
            "Test set: Difference in statistical parity outcomes between unprivileged and privileged groups = 0.000000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Classification metrics",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Test set: Classification accuracy = 0.702480\n",
            "Test set: Balanced classification accuracy = 0.500000\n",
            "Test set: Disparate impact = 1.000000\n",
            "Test set: Equal opportunity difference = 0.000000\n",
            "Test set: Average odds difference = 0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Kmd9p8Ew7yv"
      },
      "source": [
        "Non-surprisingly the metrics do not differ between the model with and without debiasing since the model without debiasing was already fair.\n",
        "\n",
        "A note on the balanced accuracy:   \n",
        "The balanced accuracy accounts for the fact that we do not have a balanced dataset and calculates the accuracy as if we had a balanced (50/50) dataset. Thus an accuracy of 0.5 resembles a guessing model, everything above 0.5 a model which has learned something (up to 1) and everything below 0.5 a model which is worse than guessing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mnCYHeU6U4W"
      },
      "source": [
        "## 4.3 Post-Processing\n",
        "\n",
        "Post-processing happens after training. The focus is on the labels predicted by the model. The training data or the learning algorithm itself is not changed. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgPSTzgbOV2M"
      },
      "source": [
        "###7. Reject Option Classification \n",
        "\n",
        "For Reject Option Classification (Kamiran et al. 2012) a simple assumption is made: Discrimination occurs most frequently where the model is most\n",
        "uncertain in its predictions, in other words around the decision boundary. Therefore the low confidence region is exploited to reduce discrimination by rejecting uncertain predictions and adjusting them accordingly. For unprivileged groups the outcome is modified positively and for privileged ones negatively. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYb7XTRo6U4W"
      },
      "source": [
        "# Load all necessary packages\n",
        "import sys\n",
        "sys.path.append(\"../\")\n",
        "from warnings import warn\n",
        "\n",
        "from aif360.datasets import BinaryLabelDataset\n",
        "from aif360.metrics import ClassificationMetric, BinaryLabelDatasetMetric\n",
        "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
        "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions\\\n",
        "        import load_preproc_data_adult, load_preproc_data_german, load_preproc_data_compas\n",
        "from aif360.algorithms.postprocessing.reject_option_classification\\\n",
        "        import RejectOptionClassification\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from ipywidgets import interactive, FloatSlider"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urI0WbojtoN-"
      },
      "source": [
        "To start, we set a few variables that we will need later on. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urPRfMWCGuwj"
      },
      "source": [
        "# Metric used (should be one of allowed_metrics, see AIF360 documentation)\n",
        "metric_name = \"Statistical parity difference\"\n",
        "\n",
        "# Upper and lower bound on the fairness metric used\n",
        "metric_ub = 0.05\n",
        "metric_lb = -0.05\n",
        "\n",
        "#random seed for calibrated equal odds prediction\n",
        "np.random.seed(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8wTfKKziav8"
      },
      "source": [
        "Again, we should measure the bias in our original data set before applying the Reject Option Classifier (ROC). For this purpose, we again measure Statistical Parity and see that the data is slightly biased with a value of -0.077 which differs from 0 (see Reweighing). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfSh_w2mHJDt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "0183279c-a8a1-4e91-80ab-4cf15d7ed739"
      },
      "source": [
        "metric_orig_train = BinaryLabelDatasetMetric(data_race_train, \n",
        "                                             unprivileged_groups=unprivileged_groups,\n",
        "                                             privileged_groups=privileged_groups)\n",
        "display(Markdown(\"#### Original training dataset\"))\n",
        "print(\"Difference in statistical parity outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Original training dataset",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Difference in statistical parity outcomes between unprivileged and privileged groups = -0.077098\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQzwrLEhjGmC"
      },
      "source": [
        "To estimate the optimal parameters for ROC, we require a validation set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2Dnb1TGIVjd"
      },
      "source": [
        "#split test set into validation and test\n",
        "np.random.seed(42)\n",
        "data_race_valid, data_race_test2 = data_race_test.split([0.5], shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsuuNcYhkj0X"
      },
      "source": [
        "Unfortunately, applying ROC to our original validation set exceeds the limitations of Google Colab's RAM. There seems to be a disadvantage here from using the AIF360 tool for bias mitigation in post-processing because one needs many resources for application. Therefore, we have to use a smaller part (70% max) of our validation set for the investigation.   \n",
        "Since ROC needs a few minutes (about 4 + 1.5 for the evaluation) to run on the whole dataset you can set `whole_dataset = False` to generate quicker results on a smaller subset of the data. Be aware that your results might differ from the ones on more data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNOrYLxzlPuA"
      },
      "source": [
        "whole_dataset = True\n",
        "\n",
        "if whole_dataset:\n",
        "  np.random.seed(3456)\n",
        "  data_ROC_valid, _ = data_race_valid.split([0.7], shuffle=True) # you can not go above 0.7 here or Google colabs will crash when fitting the ROC\n",
        "  data_ROC_test = data_race_test2.copy()\n",
        "else:\n",
        "  np.random.seed(3456)\n",
        "  data_ROC_valid, _ = data_race_valid.split([0.4], shuffle=True) # you can not go above 0.7 here or Google colabs will crash when fitting the ROC\n",
        "  data_ROC_test, _ = data_race_test2.split([0.4], shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0vhxZakki2B"
      },
      "source": [
        "We now let our random forest predict on the validation set. Because we are looking for the predictions with highest uncertainty in order to discard them later, we need the probabilities of the predictions of our model for favorable lables (which is `Crime Solved == 1`). These then only need to be transformed into the right shape which ROC expects.\n",
        "\n",
        "Remember: We assume that discrimination occurs where our probabilities of predictions are lowest, in other words where the predictions are most uncertain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fVUfAEuWZUg"
      },
      "source": [
        "data_valid_mini_pred_0_1 = rf_race.predict_proba(data_ROC_valid.features)\n",
        "data_valid_mini_pred_1 = pd.DataFrame(data_valid_mini_pred_0_1)[:][1] # = dataset_orig_valid_pred.scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztdzCUsZWbsN"
      },
      "source": [
        "data_ROC_valid_pred = data_ROC_valid.copy(deepcopy= True)\n",
        "data_ROC_valid_pred.scores = data_valid_mini_pred_1.to_numpy().reshape(-1,1) #[:,pos_ind].reshape(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tblAha9uqveV"
      },
      "source": [
        "The ROC is created using its constructor and according parameters used in AIF360's example. Then we apply the ROC on the validation set. Here the optimal threshold value at which a classification decides for category 1 or 0, is calculated. By default, this value is 0.5 for our Random Forest without fairness constraints."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvKyY53lWesk"
      },
      "source": [
        "#Estimate optimal parameters for the ROC method\n",
        "ROC = RejectOptionClassification(unprivileged_groups=unprivileged_groups, \n",
        "                                 privileged_groups=privileged_groups, \n",
        "                                 low_class_thresh=0.01, high_class_thresh=0.99,\n",
        "                                  num_class_thresh=100, num_ROC_margin=50,\n",
        "                                  metric_name=metric_name,\n",
        "                                  metric_ub=metric_ub, metric_lb=metric_lb)\n",
        "ROC = ROC.fit(data_ROC_valid, data_ROC_valid_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "365gTPflc-q4"
      },
      "source": [
        "To learn the critical region where our predictions should be rejected and adjusted, let's output the value for optimal classification threshold with fairness constraints θ. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYXqiIKGWuvj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1db7100c-b8d4-46d8-cca0-3c94ebc0a8cb"
      },
      "source": [
        "print(\"Optimal classification threshold (with fairness constraints) = %.4f\" % ROC.classification_threshold)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimal classification threshold (with fairness constraints) = 0.6930\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyD2s6HKhDpG"
      },
      "source": [
        "With the help of our validation set, we were able to find the optimal parameter: θ = 0.6930.\n",
        "\n",
        "But what exactly does this value mean? According to the authors of ROC, the critical region can be determined by a threshold value θ (Kamiran et al. 2012). The following equation makes the meaning of θ clearer (Kamiran et al. 2012): \n",
        "\\begin{equation}\n",
        "max[p(C^ + ∣X), 1 − p(C^ + ∣X)] ≤ θ \\\\(where\\ 0.5 <\n",
        "θ < 1)\n",
        "\\end{equation}\n",
        "C is the label or target class (in our case \"Crime Solved\") with values plus for favorable label (\"Crime Solved\" == 1) and minus for unfavorable label (\"Crime Solved\" == 0). X are single instances of the instance space. \n",
        "The condition states that the maximum of the probability of an instance belonging to the favorable lable and its counter probability should be smaller than or equal to θ in order to be uncertain.\n",
        "\n",
        "For example, if an instance is assigned to the favorable label by our model with probability 0.65 and counter probability of 0.35 (being assigned to the unfavorable lable), then this prediction is rejected by the ROC and adjusted accordingly because 0.65  ≤  0.0.6930 (smaller than θ). The prediction lies in the critical region and is therefore too uncertain, so it is assumed that discrimination occurs here. On the other hand, a prediction with a probability of 0.8 would not be discarded, because it is certain enough and thus, according to the author's assumption, does not discriminate.\n",
        "Related to our problem, predictions are rejected where the murder of a victim is classified as solved with a probability greater than or equal to 30.7% or less than or equal to 69.3%.   \n",
        "Disclaimer: Values above are from running ROC with 70% of the validation set!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlV_GVR_b5OP"
      },
      "source": [
        "Let's evaluate the ROC. Therefore we implemented a simple functions which outputs a given Fairness metric with and without fairness constraints and the according accuracys of the given model on which ROC is applied. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7iZ6Xei1c4p"
      },
      "source": [
        "def evaluate_ROC_helper(ROC, model, original_dataset):\n",
        "  # label prediction\n",
        "  prediction_labels = model.predict(original_dataset.features) # prediction without fairness correction\n",
        "\n",
        "  # score prediction + new StandardDataset\n",
        "  prediction = model.predict_proba(original_dataset.features)\n",
        "  data_pred = original_dataset.copy(deepcopy= True)\n",
        "  prediction_df = pd.DataFrame(prediction)[:][1]\n",
        "  data_pred.scores = prediction_df.to_numpy().reshape(-1,1) \n",
        "\n",
        "  # use ROC on StandardDataset to predict new labels\n",
        "  dataset_transf_pred = ROC.predict(data_pred)\n",
        "\n",
        "  return prediction_labels, dataset_transf_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juvuiGPd1GRJ"
      },
      "source": [
        "def evaluate_ROC(ROC, model, validation_dataset, test_dataset, metric):\n",
        "  # for metric use one of the following:\n",
        "  # \"equal opportunity\", \"disparate impact\", \"equalized odds\", \"statistical parity\"\n",
        "  # here disp. impact and statistical parity should not change due to the method used\n",
        "\n",
        "  df = pd.DataFrame(index=['Validation', 'Test'], columns=['Without fairness contraints', 'With fairness contraints', 'Accuracy (without)', 'Accuracy (with)'])\n",
        "  df.fillna(0)\n",
        "\n",
        "  prediction_validation_labels, dataset_transf_valid_pred = evaluate_ROC_helper(ROC, model, validation_dataset) \n",
        "  prediction_test_labels, dataset_transf_test_pred = evaluate_ROC_helper(ROC, model, test_dataset)\n",
        "\n",
        "  metric_value, acc = calc_fairness(validation_dataset, prediction_validation_labels, privileged_groups, unprivileged_groups, metric, False)\n",
        "  df['Without fairness contraints']['Validation'] = metric_value\n",
        "  df['Accuracy (without)']['Validation'] = acc\n",
        "\n",
        "  metric_value, acc = calc_fairness(validation_dataset, dataset_transf_valid_pred.labels.flatten(), privileged_groups, unprivileged_groups, metric, False)\n",
        "  df['With fairness contraints']['Validation'] = metric_value\n",
        "  df['Accuracy (with)']['Validation'] = acc\n",
        "\n",
        "  metric_value, acc = calc_fairness(test_dataset, prediction_test_labels, privileged_groups, unprivileged_groups, metric, False)\n",
        "  df['Without fairness contraints']['Test'] = metric_value\n",
        "  df['Accuracy (without)']['Test'] = acc\n",
        "\n",
        "  metric_value, acc = calc_fairness(test_dataset, dataset_transf_test_pred.labels.flatten(), privileged_groups, unprivileged_groups, metric, False)\n",
        "  df['With fairness contraints']['Test'] = metric_value\n",
        "  df['Accuracy (with)']['Test'] = acc\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edmJ6T7dce9t"
      },
      "source": [
        "Let's have a look at the changes of Equal Opportunity and accuracy before and after the application of ROC to our Random Forest. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VzLIWwQQCbR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "874352ba-80fb-40eb-e08c-599d28bae51b"
      },
      "source": [
        "evaluate_ROC(ROC, rf_race, data_ROC_valid, data_ROC_test, \"equal opportunity\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Without fairness contraints</th>\n",
              "      <th>With fairness contraints</th>\n",
              "      <th>Accuracy (without)</th>\n",
              "      <th>Accuracy (with)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Validation</th>\n",
              "      <td>-0.067952</td>\n",
              "      <td>-0.00756258</td>\n",
              "      <td>0.757705</td>\n",
              "      <td>0.711929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Test</th>\n",
              "      <td>-0.0674639</td>\n",
              "      <td>-0.00614594</td>\n",
              "      <td>0.758277</td>\n",
              "      <td>0.711531</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Without fairness contraints  ... Accuracy (with)\n",
              "Validation                   -0.067952  ...        0.711929\n",
              "Test                        -0.0674639  ...        0.711531\n",
              "\n",
              "[2 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ9jeuMHpASS"
      },
      "source": [
        "We can see that Equal Opportunity can be improved by an absolute value of 0.061 using the ROC on the test set which is equal to a relative improvement of 91%. Thereby, the accuracy of our classifier decreases by an absolute value of 0.047 (6.2% in relative terms), which is not remarkable compared to the improvement in the fairness measure. Thus, according to equal opportunity, we could mitigate the bias a fair bit and are now nearly at a value of 0 for Equal Opportunity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9jIJjsVrJPV"
      },
      "source": [
        "The greatest success was achieved with Equalized Odds, where the value could be improved by an absolute value of 0.09 and a relative value of 90% in the test set. This is a great improvement since Equalized Odds is now nearly equal to 0. Accuracies are the same as above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1KZ-JLtzhUb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "e2d5b077-9ecf-4190-da65-418d2414de14"
      },
      "source": [
        "evaluate_ROC(ROC, rf_race, data_ROC_valid, data_ROC_test, \"equalized odds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Without fairness contraints</th>\n",
              "      <th>With fairness contraints</th>\n",
              "      <th>Accuracy (without)</th>\n",
              "      <th>Accuracy (with)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Validation</th>\n",
              "      <td>-0.109327</td>\n",
              "      <td>-0.0138436</td>\n",
              "      <td>0.757705</td>\n",
              "      <td>0.711929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Test</th>\n",
              "      <td>-0.102936</td>\n",
              "      <td>-0.00753336</td>\n",
              "      <td>0.758277</td>\n",
              "      <td>0.711531</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Without fairness contraints  ... Accuracy (with)\n",
              "Validation                   -0.109327  ...        0.711929\n",
              "Test                         -0.102936  ...        0.711531\n",
              "\n",
              "[2 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV-i0c7OqFn9"
      },
      "source": [
        "For the Disparate Impact, we could not achieve any improvement with the ROC. Accuracies are the same as above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9z4zRDZzhi4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "704ae753-479c-4f96-b35e-2b197990e534"
      },
      "source": [
        "evaluate_ROC(ROC, rf_race, data_ROC_valid, data_ROC_test, \"disparate impact\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Without fairness contraints</th>\n",
              "      <th>With fairness contraints</th>\n",
              "      <th>Accuracy (without)</th>\n",
              "      <th>Accuracy (with)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Validation</th>\n",
              "      <td>0.901293</td>\n",
              "      <td>0.901293</td>\n",
              "      <td>0.757705</td>\n",
              "      <td>0.711929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Test</th>\n",
              "      <td>0.893344</td>\n",
              "      <td>0.893344</td>\n",
              "      <td>0.758277</td>\n",
              "      <td>0.711531</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Without fairness contraints  ... Accuracy (with)\n",
              "Validation                    0.901293  ...        0.711929\n",
              "Test                          0.893344  ...        0.711531\n",
              "\n",
              "[2 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0GJbJ2YqqKU"
      },
      "source": [
        "Statistical Parity also does not improve with the ROC, while accuracies are the same as above. Since Disparate Impact and Statistical Parity overlap, this result is not surprising. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVF_oFO5zhwc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "4e376093-b61e-42fe-e22f-c26b5dd6004f"
      },
      "source": [
        "evaluate_ROC(ROC, rf_race, data_ROC_valid, data_ROC_test, \"statistical parity\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Without fairness contraints</th>\n",
              "      <th>With fairness contraints</th>\n",
              "      <th>Accuracy (without)</th>\n",
              "      <th>Accuracy (with)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Validation</th>\n",
              "      <td>-0.0727886</td>\n",
              "      <td>-0.0727886</td>\n",
              "      <td>0.757705</td>\n",
              "      <td>0.711929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Test</th>\n",
              "      <td>-0.0793057</td>\n",
              "      <td>-0.0793057</td>\n",
              "      <td>0.758277</td>\n",
              "      <td>0.711531</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Without fairness contraints  ... Accuracy (with)\n",
              "Validation                  -0.0727886  ...        0.711929\n",
              "Test                        -0.0793057  ...        0.711531\n",
              "\n",
              "[2 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glWwNnlUm4nP"
      },
      "source": [
        "Our results show that applying the ROC to mitigate the bias from the data and our model was only successful for two of the four Fairness metrics: For Equal Opportunity and Equalized Odds. No improvement was achieved for Statistical Parity and Disparate Impact. After the application of the ROC to our model accuracy decreased slightly by 0.047 = 4.7% using the test set. \n",
        "The application of ROC to our Random Forest does not suggest an over- or underfitting, since the accuracies of validation and test set are similar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2z-xbmfZ4Hkz"
      },
      "source": [
        "# 5. Results of applying the AIF360 Tool <a class=\"anchor\" id=\"chapter5\"></a>\n",
        "For our studies, we trained a Random Forest model with the Homicide dataset. We examined the model and the dataset for bias and were able to measure a slight bias in the bias detection section through our fairness metrics. Here, we were able to determine that all metrics indicate that the model and dataset favor the privileged group and thus the white group. Which in our case means that crimes are more likely to be solved when victims are white.  \n",
        "\n",
        "We then investigated how to mitigate the measured bias using various bias mitigation algorithms of the AIF360 tool. The results were ambivalent. Two of the four algorithms tested were able to achieve a positive effect and reduce the bias without a strong decrease in the accuracy of the model.   \n",
        "* In pre-processing, the dataset successfully satisfied Statistical Parity after **Reweighing** was applied. The performance of the Reweighing method was satisfactory, as no long run times resulted despite the large data set. The application was simple and no major problems were encountered.\n",
        "* **Disparate Impact Remover** as our second algorithm to mitigate bias in the pre-processing fails to produce profound results on the whole dataset. DIR is a computational intensive method (time) when dealing with a lot of data. However it did show effects on a subset of the data. This might be a hint that DIR is especially good for smaller datasets. Further exploration is needed on this topic.\n",
        "* We can not make any assumptions about the **Adversarial Debiasing** algorithm since surprisingly our base deep neural network didn't learn anything in the end. More options to individualize the DNN/Adverserial Debiasing in the first step need to be implemented in the AIF360 tool to be able to work with it on a professional level.\n",
        "* Last, while **Reject Option Classification** as a post-processing technique could satisfy Equal Opportunity and Equalized Odds, Statistical Parity and Disparate Impact could not. After applying the Reject Option Classification, the accuracy of the model decreased a little as expected. The performance of the Reject Option Classification method leaves a lot to be desired: The application requires a lot of memory resources, so that only a part of our dataset could be used for this approach in Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kyHiMiO54wk"
      },
      "source": [
        "# 6. Conclusion <a class=\"anchor\" id=\"chapter6\"></a>\n",
        "\n",
        "**How did we measure fairness?**\n",
        "\n",
        "**How did we mitigate bias in fairness while still producing a valid model?**\n",
        "\n",
        "**And how did the AIF360 Tool perform?**\n",
        "\n",
        "We have seen how Fairness can be measured with the AIF360 tool. Applying the Fairness metrics to our data set and model was easy to use and did not cause any problems. We were able to demonstrate a bias in the Homicide data set and our Random Forest model.\n",
        "\n",
        "During the application of the AIF360 tool, we noticed that the individual examples and algorithms in the source code were not sufficiently documented, which often lead to difficulties in understanding and transferring them to our random forest model. Unfortunately, certain methods such as Adversarial Debiasing, were not successful in mitigating the bias. Nevertheless, we were able to successfully mitigate the bias in our dataset using other methods. Another obstacle when working with the AIF360 tool is the StandardDataset provided by the tool. With the StandardDataset, one cannot use the widely known methods (such as training a DNN on a dataframe), but is forced to use the modified methods implemented in the AIF360 tool. This makes the tool less flexible to work with and harder to understand for junior data scientists.\n",
        "\n",
        "For the further development of the extensible AIF360 tool, we would like to see more extensive documentation, more flexibility and the further development of certain methods like Adversarial Debiasing. \n",
        " \n",
        "Nevertheless, the AIF360 toolkit is a great approach to address the problem of fairness in machine learning models. In our case, two methods of the tool successfully mitigated the measured bias while the model remained valid. We presume that the success of the tool's methods is highly dependent on the use case and dataset. \n",
        "\n",
        "Since all bias mitigation algorithms attempt to satisfy a particular Fairness metric and are designed to do so, the bias mitigation method highly depends on the quality of the Fairness metric chosen. If the Fairness metric per se does not properly map the bias from the data, then no real improvement will be achieved by our bias mitigation algorithms built on the metric - even though the fairness metric indicates a supposed improvement.\n",
        "\n",
        "In general, a model can only be as good as the data on which it makes predictions: \n",
        "```\n",
        "\"Evidence-based decision-making is only as reliable as the evidence on which \n",
        "it is based, and high quality examples are critically important\n",
        " to machine learning.\" (Barocas et al. 2017)\n",
        "```\n",
        "If the data we work with is biased, so are our predictions. Even if bias can be mitigated with the methods applied, it does not gurantee that bias is completely removed from the data, and predictions do not discriminate.  \n",
        "For this reason, the following questions should always be asked when building a model (see talk on \"The Trouble with Bias\" by Kate Crawford (2017), available via Youtube):\n",
        "```\n",
        "Are there some systems we just shouldn't build? \n",
        "Who built the model? \n",
        "Who could be harmed by the model? \n",
        "Who will benefit from the model?\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Sk7KlbR4QGn"
      },
      "source": [
        "# 7. References <a class=\"anchor\" id=\"chapter7\"></a>\n",
        "* Bellamy et al. (2018). AI Fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias. arXiv preprint arXiv:1810.01943.\n",
        "* Barocas et al. (2017). Fairness in machine learning. Nips tutorial, 1, 2017.\n",
        "* Mehrabi et al. (2021). A survey on bias and fairness in machine learning. ACM Computing Surveys (CSUR), 54(6), 1-35.\n",
        "* Verma & Rubin (2018). Fairness definitions explained. In 2018 ieee/acm international workshop on software fairness (fairware) (pp. 1-7). IEEE.\n",
        " \n",
        "**Papers on bias mitigation algorithms:** \n",
        " \n",
        "* Calmon et al. (2017). Optimized pre-processing for discrimination prevention. In Proceedings of the 31st International Conference on Neural Information Processing Systems (pp. 3995-4004).\n",
        "* Feldman et al. (2015). Certifying and removing disparate impact. In proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining (pp. 259-268).\n",
        "* Hardt et al. (2016). Equality of opportunity in supervised learning. Advances in neural information processing systems, 29, 3315-3323.\n",
        "* Kamiran & Calders (2012). Data preprocessing techniques for classification without discrimination. Knowledge and Information Systems, 33(1), 1-33.\n",
        "* Kamiran et al. (2012). Decision theory for discrimination-aware classification. In 2012 IEEE 12th International Conference on Data Mining (pp. 924-929). IEEE.\n",
        "* Kamishima et al. (2012). Fairness-aware classifier with prejudice remover regularizer. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases (pp. 35-50). Springer, Berlin, Heidelberg.\n",
        "* Pleiss et al. (2017). On fairness and calibration. arXiv preprint arXiv:1709.02012.\n",
        "* Zhang et al. (2018). Mitigating unwanted biases with adversarial learning. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society (pp. 335-340).\n",
        "* Zemel et al. (2013). Learning fair representations. In International conference on machine learning (pp. 325-333). PMLR.\n",
        " \n",
        "**Data and documentations:**\n",
        "* Homicide data: https://www.kaggle.com/murderaccountability/homicide-reports (under licence: https://creativecommons.org/licenses/by-sa/4.0/) [Last visited: 02.08.2021].\n",
        "* IBM Research Trusted AI, official webseit of the AIF360 tool kit: https://aif360.mybluemix.net [Last visited: 02.08.2021].\n",
        "* AIF360 documentation: https://aif360.readthedocs.io/en/latest/ [Last visited: 02.08.2021].\n",
        "* GitHub repository of AIF360: https://github.com/Trusted-AI/AIF360 [Last visited: 02.08.2021].\n",
        "* Bianca Zimmer's GitHub repository for data and pictures: https://github.com/BiancaZimmer/Stat-ML-Fairness.git [Last visited: 02.08.2021]."
      ]
    }
  ]
}